{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALEX NET\n",
    "Edition: Alex try-Copy10 <br/>\n",
    "Branch from Alex try-Copy9、Copy4(IO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROWS = 227\n",
    "COLS = 227"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 執行訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import random\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_dict= { 0:'free', 1:'computer', 2:'cellphone', 3:'coin', 4:'ruler', \n",
    "        5:'thermos-bottle', 6:'whiteboard-pen', 7:'whiteboard-eraser',\n",
    "        8:'pen', 9:'cup', 10:'remote-control-TV', 11:'remote-control-AC',\n",
    "        12:'switch', 13:'windows', 14:'fridge', 15:'cupboard', 16:'water-tap',\n",
    "        17:'toy', 18:'kettle', 19:'bottle', 20:'cookie', 21:'book',\n",
    "        22:'magnet', 23:'lamp-switch'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建構Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_train_history(train_history,train,validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train','validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def save_train_history(train_history,train,validation):\n",
    "    my_trainList.append(train_history.history[train])\n",
    "    my_validList.append(train_history.history[validation])\n",
    "\n",
    "def save_train_history2(train_history,train,validation):\n",
    "    my_trainList2.append(train_history.history[train])\n",
    "    my_validList2.append(train_history.history[validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 55, 55, 96)        34944     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 24)                98328     \n",
      "=================================================================\n",
      "Total params: 58,379,672\n",
      "Trainable params: 58,379,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense,Flatten,Dropout  \n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D  \n",
    "from keras.utils.np_utils import to_categorical  \n",
    "import numpy as np  \n",
    "seed = 7  \n",
    "np.random.seed(seed)  \n",
    "  \n",
    "model = Sequential()  \n",
    "model.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(227,227,3),padding='valid',activation='relu',kernel_initializer='uniform'))  \n",
    "model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model.add(Conv2D(256,(5,5),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(24,activation='softmax'))\n",
    "#model.add(Dense(1000,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])  \n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練過程\n",    
    "### 每一輪 跑10組data set (這10組data set隨機餵進去)",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_trainList=[]\n",
    "my_validList=[]\n",
    "my_trainList2=[]\n",
    "my_validList2=[]\n",
    "\n",
    "testaccList=[]\n",
    "teslossList=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(round: 0 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 2s - loss: 1.9542 - acc: 0.4728 - val_loss: 1.5817 - val_acc: 0.6035\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 2.3655 - acc: 0.4167 - val_loss: 1.9067 - val_acc: 0.4053\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 1.9723 - acc: 0.4405 - val_loss: 1.8543 - val_acc: 0.4776\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 1.7644 - acc: 0.5275 - val_loss: 1.9319 - val_acc: 0.5616\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 2.4043 - acc: 0.4437 - val_loss: 1.7181 - val_acc: 0.4410\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 1.6058 - acc: 0.4622 - val_loss: 1.5326 - val_acc: 0.4195\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 2.4318 - acc: 0.3820 - val_loss: 2.0124 - val_acc: 0.3930\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 1.9954 - acc: 0.4332 - val_loss: 1.7231 - val_acc: 0.5514\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 1.9381 - acc: 0.4183 - val_loss: 1.7109 - val_acc: 0.4873\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 1.7630 - acc: 0.4918 - val_loss: 1.8754 - val_acc: 0.4904\n",
      "(Average) Test Accuracy= 0.493173524783\n",
      "(Average) Test Loss= 1.81639479302\n",
      "\n",
      "(round: 1 )\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 2.1077 - acc: 0.3933 - val_loss: 1.7375 - val_acc: 0.4428\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 1.6779 - acc: 0.4986 - val_loss: 1.6568 - val_acc: 0.4904\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 1.9481 - acc: 0.4649 - val_loss: 1.8351 - val_acc: 0.5163\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 1.9406 - acc: 0.4546 - val_loss: 1.6793 - val_acc: 0.5445\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 1.5532 - acc: 0.4883 - val_loss: 1.4573 - val_acc: 0.4228\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 1.9933 - acc: 0.4148 - val_loss: 1.7823 - val_acc: 0.4508\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 1.8409 - acc: 0.4236 - val_loss: 1.6049 - val_acc: 0.5000\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 1.6451 - acc: 0.4536 - val_loss: 1.5126 - val_acc: 0.5022\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 1.6039 - acc: 0.5671 - val_loss: 1.4832 - val_acc: 0.5783\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 1.6412 - acc: 0.5260 - val_loss: 1.7434 - val_acc: 0.5526\n",
      "(Average) Test Accuracy= 0.452443586152\n",
      "(Average) Test Loss= 1.81090237622\n",
      "\n",
      "(round: 2 )\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 1.6020 - acc: 0.4986 - val_loss: 1.5408 - val_acc: 0.5287\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 1.4807 - acc: 0.5722 - val_loss: 1.1651 - val_acc: 0.6843\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 1.9082 - acc: 0.4395 - val_loss: 2.1613 - val_acc: 0.1907\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 1.6499 - acc: 0.4773 - val_loss: 1.7019 - val_acc: 0.4356\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 1.6112 - acc: 0.4795 - val_loss: 1.1473 - val_acc: 0.7055\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 1.5238 - acc: 0.5106 - val_loss: 1.3359 - val_acc: 0.6119\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 1.6403 - acc: 0.5320 - val_loss: 1.3634 - val_acc: 0.6186\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 1.4802 - acc: 0.5294 - val_loss: 1.1116 - val_acc: 0.5906\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 1.7891 - acc: 0.4705 - val_loss: 1.7273 - val_acc: 0.4573\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 1.7151 - acc: 0.4678 - val_loss: 1.3879 - val_acc: 0.5109\n",
      "(Average) Test Accuracy= 0.424061278806\n",
      "(Average) Test Loss= 1.81108034427\n",
      "\n",
      "(round: 3 )\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 1.6372 - acc: 0.4894 - val_loss: 1.3345 - val_acc: 0.5169\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 1.4752 - acc: 0.5189 - val_loss: 1.2309 - val_acc: 0.5833\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 1.3927 - acc: 0.5344 - val_loss: 1.3604 - val_acc: 0.4430\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 1.6734 - acc: 0.5071 - val_loss: 1.4487 - val_acc: 0.5549\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 1.4806 - acc: 0.5317 - val_loss: 1.2328 - val_acc: 0.63700.5\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 1.5174 - acc: 0.5060 - val_loss: 1.2833 - val_acc: 0.5328\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 1.3447 - acc: 0.5197 - val_loss: 1.1579 - val_acc: 0.5900\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 1.7735 - acc: 0.5231 - val_loss: 1.1591 - val_acc: 0.6766\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 1.4111 - acc: 0.6095 - val_loss: 1.4656 - val_acc: 0.4823\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 1.3330 - acc: 0.5756 - val_loss: 1.1336 - val_acc: 0.6607\n",
      "(Average) Test Accuracy= 0.397690715926\n",
      "(Average) Test Loss= 1.79106153704\n",
      "\n",
      "(round: 4 )\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 1.4784 - acc: 0.5427 - val_loss: 1.1300 - val_acc: 0.5939\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.9666 - acc: 0.6737 - val_loss: 0.8890 - val_acc: 0.6913\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 1.7131 - acc: 0.4883 - val_loss: 1.2082 - val_acc: 0.5593\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 1.2252 - acc: 0.6130 - val_loss: 1.0831 - val_acc: 0.6567\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 1.5883 - acc: 0.5336 - val_loss: 1.4322 - val_acc: 0.5813\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 1.3164 - acc: 0.5993 - val_loss: 0.8736 - val_acc: 0.7021\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056/1056 [==============================] - 1s - loss: 1.2583 - acc: 0.5871 - val_loss: 1.1608 - val_acc: 0.6250\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 1.3789 - acc: 0.5552 - val_loss: 1.4841 - val_acc: 0.4454\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 1.4713 - acc: 0.5741 - val_loss: 1.1165 - val_acc: 0.6907\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 1.0044 - acc: 0.6994 - val_loss: 1.4663 - val_acc: 0.4621\n",
      "(Average) Test Accuracy= 0.359817719902\n",
      "(Average) Test Loss= 1.87908112203\n",
      "\n",
      "(round: 5 )\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 1.2268 - acc: 0.5925 - val_loss: 0.8414 - val_acc: 0.7123\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 1.2360 - acc: 0.6298 - val_loss: 1.3722 - val_acc: 0.5255\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.8628 - acc: 0.7253 - val_loss: 0.7383 - val_acc: 0.7652\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 1.1550 - acc: 0.6124 - val_loss: 0.7980 - val_acc: 0.7483\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 1.5044 - acc: 0.5590 - val_loss: 1.2355 - val_acc: 0.5752\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 1.3964 - acc: 0.6222 - val_loss: 0.8711 - val_acc: 0.7235\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 1.3440 - acc: 0.5773 - val_loss: 0.9154 - val_acc: 0.7050\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 1.3797 - acc: 0.5690 - val_loss: 1.0256 - val_acc: 0.6356\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 1.2565 - acc: 0.5880 - val_loss: 0.8943 - val_acc: 0.7380\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 1.3045 - acc: 0.5980 - val_loss: 0.9175 - val_acc: 0.6915\n",
      "(Average) Test Accuracy= 0.424793241931\n",
      "(Average) Test Loss= 1.76150904476\n",
      "\n",
      "(round: 6 )\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 1.1152 - acc: 0.6372 - val_loss: 0.8172 - val_acc: 0.7380\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.8464 - acc: 0.7022 - val_loss: 0.6517 - val_acc: 0.7617\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 1.0513 - acc: 0.6712 - val_loss: 1.0643 - val_acc: 0.6849\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 1.0041 - acc: 0.6790 - val_loss: 0.7473 - val_acc: 0.7462\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.9299 - acc: 0.7278 - val_loss: 0.9676 - val_acc: 0.7096\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 1.2082 - acc: 0.6104 - val_loss: 0.9848 - val_acc: 0.6653\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 1.0042 - acc: 0.6841 - val_loss: 0.8448 - val_acc: 0.6667\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 1.0486 - acc: 0.6727 - val_loss: 1.1119 - val_acc: 0.6396\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 1.2848 - acc: 0.5898 - val_loss: 0.9715 - val_acc: 0.6705\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 1.4019 - acc: 0.5697 - val_loss: 1.2190 - val_acc: 0.5671\n",
      "(Average) Test Accuracy= 0.423477368948\n",
      "(Average) Test Loss= 2.02178723437\n",
      "\n",
      "(round: 7 )\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 1.0410 - acc: 0.6752 - val_loss: 0.6291 - val_acc: 0.7879\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.9728 - acc: 0.6779 - val_loss: 0.8581 - val_acc: 0.7015\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 1.1038 - acc: 0.6234 - val_loss: 0.7955 - val_acc: 0.7088\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.9600 - acc: 0.6743 - val_loss: 0.7698 - val_acc: 0.7380\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.7516 - acc: 0.7399 - val_loss: 0.6193 - val_acc: 0.7953\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 1.1606 - acc: 0.6348 - val_loss: 0.7529 - val_acc: 0.7542\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.8926 - acc: 0.7285 - val_loss: 0.6931 - val_acc: 0.7904\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.8907 - acc: 0.7123 - val_loss: 0.8773 - val_acc: 0.6541\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 1.3623 - acc: 0.5849 - val_loss: 0.9945 - val_acc: 0.6931\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.9446 - acc: 0.6915 - val_loss: 0.6727 - val_acc: 0.7898\n",
      "(Average) Test Accuracy= 0.451021521067\n",
      "(Average) Test Loss= 1.74774162539\n",
      "\n",
      "(round: 8 )\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 1.2092 - acc: 0.6055 - val_loss: 1.3022 - val_acc: 0.5328\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.7096 - acc: 0.7584 - val_loss: 0.5279 - val_acc: 0.8221\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.8477 - acc: 0.7359 - val_loss: 0.6507 - val_acc: 0.7958\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.8717 - acc: 0.6960 - val_loss: 0.6141 - val_acc: 0.8144\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.9954 - acc: 0.6734 - val_loss: 0.7038 - val_acc: 0.7241\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.7962 - acc: 0.7481 - val_loss: 0.5567 - val_acc: 0.8157\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.7460 - acc: 0.7560 - val_loss: 0.6190 - val_acc: 0.7740\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 1.1772 - acc: 0.6282 - val_loss: 1.0525 - val_acc: 0.6789\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 1.1766 - acc: 0.6327 - val_loss: 0.7918 - val_acc: 0.7415\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.8479 - acc: 0.7141 - val_loss: 0.7029 - val_acc: 0.7463\n",
      "(Average) Test Accuracy= 0.41828215908\n",
      "(Average) Test Loss= 2.0194674707\n",
      "\n",
      "(round: 9 )\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.6918 - acc: 0.7784 - val_loss: 0.5132 - val_acc: 0.8258\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 1.1050 - acc: 0.6254 - val_loss: 0.8599 - val_acc: 0.6858\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.8081 - acc: 0.7333 - val_loss: 0.6600 - val_acc: 0.7948\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.6842 - acc: 0.7842 - val_loss: 0.3633 - val_acc: 0.9007\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.6820 - acc: 0.7949 - val_loss: 0.5267 - val_acc: 0.8434\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 1.0571 - acc: 0.6434 - val_loss: 0.9730 - val_acc: 0.6911\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 1.0761 - acc: 0.6816 - val_loss: 0.7275 - val_acc: 0.7313\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.7074 - acc: 0.7785 - val_loss: 0.5326 - val_acc: 0.8322\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.7970 - acc: 0.7570 - val_loss: 0.5776 - val_acc: 0.8168\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.9380 - acc: 0.6762 - val_loss: 0.6395 - val_acc: 0.7924\n",
      "(Average) Test Accuracy= 0.42457647586\n",
      "(Average) Test Loss= 1.90287112861\n",
      "\n",
      "(round: 10 )\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.6808 - acc: 0.7765 - val_loss: 0.4778 - val_acc: 0.8523\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.6306 - acc: 0.7987 - val_loss: 0.4382 - val_acc: 0.8557\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.5947 - acc: 0.7945 - val_loss: 0.2793 - val_acc: 0.9075\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 1.0366 - acc: 0.6831 - val_loss: 0.9337 - val_acc: 0.6992\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.6902 - acc: 0.7735 - val_loss: 0.5759 - val_acc: 0.8138\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.5664 - acc: 0.8152 - val_loss: 0.4761 - val_acc: 0.8510\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.9204 - acc: 0.7003 - val_loss: 0.7189 - val_acc: 0.7548\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.8955 - acc: 0.7403 - val_loss: 0.6018 - val_acc: 0.7761\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.7382 - acc: 0.7537 - val_loss: 0.7759 - val_acc: 0.7076\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.8591 - acc: 0.7202 - val_loss: 1.5055 - val_acc: 0.4672\n",
      "(Average) Test Accuracy= 0.404941240095\n",
      "(Average) Test Loss= 1.93188826458\n",
      "\n",
      "(round: 11 )\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.7095 - acc: 0.7795 - val_loss: 0.4570 - val_acc: 0.8468\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.8569 - acc: 0.7197 - val_loss: 0.6954 - val_acc: 0.7744\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.9643 - acc: 0.7060 - val_loss: 0.5769 - val_acc: 0.8035\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.5355 - acc: 0.8171 - val_loss: 0.4970 - val_acc: 0.8121\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.6705 - acc: 0.7867 - val_loss: 0.8266 - val_acc: 0.7165\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.9547 - acc: 0.7166 - val_loss: 0.5732 - val_acc: 0.7797\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.4796 - acc: 0.8279 - val_loss: 0.7642 - val_acc: 0.7295\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.7963 - acc: 0.7503 - val_loss: 0.6370 - val_acc: 0.8109\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.5880 - acc: 0.8044 - val_loss: 0.4469 - val_acc: 0.8561\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.6358 - acc: 0.7955 - val_loss: 0.4558 - val_acc: 0.8333\n",
      "(Average) Test Accuracy= 0.439391479065\n",
      "(Average) Test Loss= 1.86907969888\n",
      "\n",
      "(round: 12 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.4321 - acc: 0.8532 - val_loss: 0.4024 - val_acc: 0.8712\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.6531 - acc: 0.7853 - val_loss: 0.5988 - val_acc: 0.7761\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.7333 - acc: 0.7650 - val_loss: 0.7161 - val_acc: 0.7686\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.4913 - acc: 0.8280 - val_loss: 0.4350 - val_acc: 0.8389\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.6639 - acc: 0.7627 - val_loss: 0.6559 - val_acc: 0.7778\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.5281 - acc: 0.8433 - val_loss: 0.3943 - val_acc: 0.8767\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.6552 - acc: 0.7941 - val_loss: 0.5741 - val_acc: 0.7966\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.8711 - acc: 0.7218 - val_loss: 0.9949 - val_acc: 0.6606\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.5943 - acc: 0.8194 - val_loss: 0.4760 - val_acc: 0.8378\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.6396 - acc: 0.7926 - val_loss: 0.7223 - val_acc: 0.8144\n",
      "(Average) Test Accuracy= 0.437866916226\n",
      "(Average) Test Loss= 1.94874019953\n",
      "\n",
      "(round: 13 )\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.6806 - acc: 0.7771 - val_loss: 0.5435 - val_acc: 0.7797\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.4062 - acc: 0.8671 - val_loss: 0.3625 - val_acc: 0.8914\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.7253 - acc: 0.7683 - val_loss: 0.5179 - val_acc: 0.8253\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.4738 - acc: 0.8622 - val_loss: 0.2752 - val_acc: 0.9110\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.5112 - acc: 0.8347 - val_loss: 0.3929 - val_acc: 0.8691\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.5933 - acc: 0.8050 - val_loss: 1.3450 - val_acc: 0.5364\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.7588 - acc: 0.7936 - val_loss: 0.4714 - val_acc: 0.8447\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1966/1966 [==============================] - 2s - loss: 0.8157 - acc: 0.7467 - val_loss: 0.5649 - val_acc: 0.8130\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.7471 - acc: 0.7541 - val_loss: 0.4652 - val_acc: 0.8308\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.5316 - acc: 0.8292 - val_loss: 0.4524 - val_acc: 0.8589\n",
      "(Average) Test Accuracy= 0.429858407633\n",
      "(Average) Test Loss= 1.97300922008\n",
      "\n",
      "(round: 14 )\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.4739 - acc: 0.8419 - val_loss: 0.3951 - val_acc: 0.8561\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.6461 - acc: 0.7781 - val_loss: 0.6324 - val_acc: 0.7860\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.4649 - acc: 0.8465 - val_loss: 0.3986 - val_acc: 0.8739\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.4237 - acc: 0.8656 - val_loss: 0.2036 - val_acc: 0.9384\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.6167 - acc: 0.8079 - val_loss: 0.5099 - val_acc: 0.8391\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.6505 - acc: 0.7792 - val_loss: 0.4746 - val_acc: 0.8305\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.7338 - acc: 0.7706 - val_loss: 0.6781 - val_acc: 0.7683\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.7453 - acc: 0.7541 - val_loss: 0.5082 - val_acc: 0.8408\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.4859 - acc: 0.8372 - val_loss: 0.4053 - val_acc: 0.8557\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.4512 - acc: 0.8513 - val_loss: 0.3529 - val_acc: 0.8914\n",
      "(Average) Test Accuracy= 0.46347401929\n",
      "(Average) Test Loss= 1.97003177126\n",
      "\n",
      "(round: 15 )\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.3863 - acc: 0.8716 - val_loss: 0.3184 - val_acc: 0.8826\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.5324 - acc: 0.8240 - val_loss: 0.4846 - val_acc: 0.8159\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.3204 - acc: 0.8873 - val_loss: 0.4401 - val_acc: 0.8561\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.6430 - acc: 0.8021 - val_loss: 0.4654 - val_acc: 0.8467\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.4884 - acc: 0.8532 - val_loss: 0.3932 - val_acc: 0.8750\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.6574 - acc: 0.7904 - val_loss: 0.5138 - val_acc: 0.8435\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.4346 - acc: 0.8553 - val_loss: 0.3804 - val_acc: 0.8801\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.4424 - acc: 0.8540 - val_loss: 0.3953 - val_acc: 0.8769\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.6050 - acc: 0.7930 - val_loss: 0.5702 - val_acc: 0.7924\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.5885 - acc: 0.8022 - val_loss: 0.4655 - val_acc: 0.8734\n",
      "(Average) Test Accuracy= 0.454908984138\n",
      "(Average) Test Loss= 2.00318046401\n",
      "\n",
      "(round: 16 )\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.4473 - acc: 0.8570 - val_loss: 0.3489 - val_acc: 0.8864\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.3348 - acc: 0.8856 - val_loss: 0.3325 - val_acc: 0.8979\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.4969 - acc: 0.8284 - val_loss: 0.5259 - val_acc: 0.8559\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.5267 - acc: 0.8127 - val_loss: 0.4340 - val_acc: 0.8756\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.4141 - acc: 0.8733 - val_loss: 0.3566 - val_acc: 0.8926\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.3442 - acc: 0.8829 - val_loss: 0.3144 - val_acc: 0.8965\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.4758 - acc: 0.8521 - val_loss: 0.6322 - val_acc: 0.7778\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.6787 - acc: 0.7981 - val_loss: 0.4530 - val_acc: 0.8455\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.3975 - acc: 0.8836 - val_loss: 0.1788 - val_acc: 0.9418\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.4771 - acc: 0.8248 - val_loss: 0.4251 - val_acc: 0.8263\n",
      "(Average) Test Accuracy= 0.48269294098\n",
      "(Average) Test Loss= 2.02638436064\n",
      "\n",
      "(round: 17 )\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.5005 - acc: 0.8316 - val_loss: 0.3726 - val_acc: 0.8699\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.5006 - acc: 0.8515 - val_loss: 0.3223 - val_acc: 0.8926\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.3144 - acc: 0.8949 - val_loss: 0.3313 - val_acc: 0.9066\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.2882 - acc: 0.8999 - val_loss: 0.3330 - val_acc: 0.8919\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.3799 - acc: 0.8703 - val_loss: 0.3549 - val_acc: 0.9015\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.4503 - acc: 0.8464 - val_loss: 0.3542 - val_acc: 0.8955\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.5375 - acc: 0.8186 - val_loss: 0.4392 - val_acc: 0.8690\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.3994 - acc: 0.8646 - val_loss: 0.3900 - val_acc: 0.8697\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.4992 - acc: 0.8418 - val_loss: 0.4589 - val_acc: 0.8305\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.2350 - acc: 0.9161 - val_loss: 0.1726 - val_acc: 0.9384\n",
      "(Average) Test Accuracy= 0.490592270843\n",
      "(Average) Test Loss= 1.94328016655\n",
      "\n",
      "(round: 18 )\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.3948 - acc: 0.8674 - val_loss: 0.4272 - val_acc: 0.8659\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.4309 - acc: 0.8769 - val_loss: 0.5044 - val_acc: 0.8136\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.2765 - acc: 0.9025 - val_loss: 0.3152 - val_acc: 0.9015\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.5030 - acc: 0.8362 - val_loss: 0.3913 - val_acc: 0.8679\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.2892 - acc: 0.9092 - val_loss: 0.1753 - val_acc: 0.9521\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.5875 - acc: 0.8240 - val_loss: 0.3568 - val_acc: 0.9055\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.3550 - acc: 0.8864 - val_loss: 0.3275 - val_acc: 0.8939\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.3781 - acc: 0.8725 - val_loss: 0.3475 - val_acc: 0.8725\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.4245 - acc: 0.8557 - val_loss: 0.4064 - val_acc: 0.8952\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.3618 - acc: 0.8856 - val_loss: 0.2930 - val_acc: 0.9069\n",
      "(Average) Test Accuracy= 0.487873942629\n",
      "(Average) Test Loss= 2.07638562363\n",
      "\n",
      "(round: 19 )\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.4387 - acc: 0.8591 - val_loss: 0.3885 - val_acc: 0.8659\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.4580 - acc: 0.8674 - val_loss: 0.3390 - val_acc: 0.8902\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.4533 - acc: 0.8549 - val_loss: 0.4097 - val_acc: 0.8582\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.2517 - acc: 0.9074 - val_loss: 0.3112 - val_acc: 0.9159\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.4071 - acc: 0.8652 - val_loss: 0.4614 - val_acc: 0.8220\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.3771 - acc: 0.8664 - val_loss: 0.3701 - val_acc: 0.9005\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.3330 - acc: 0.8859 - val_loss: 0.2975 - val_acc: 0.8960\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.2230 - acc: 0.9281 - val_loss: 0.1787 - val_acc: 0.9384\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.5316 - acc: 0.8415 - val_loss: 0.3710 - val_acc: 0.8996\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.2770 - acc: 0.9025 - val_loss: 0.3850 - val_acc: 0.8864\n",
      "(Average) Test Accuracy= 0.492160990867\n",
      "(Average) Test Loss= 2.03713365094\n",
      "\n",
      "(round: 20 )\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.2566 - acc: 0.9195 - val_loss: 0.4560 - val_acc: 0.8826\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.3472 - acc: 0.8917 - val_loss: 0.3747 - val_acc: 0.8602\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.4513 - acc: 0.8505 - val_loss: 0.3949 - val_acc: 0.8679\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.4406 - acc: 0.8598 - val_loss: 0.9226 - val_acc: 0.7011\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.3885 - acc: 0.8699 - val_loss: 0.4442 - val_acc: 0.8821\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.2883 - acc: 0.9077 - val_loss: 0.3037 - val_acc: 0.9027\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.3644 - acc: 0.8677 - val_loss: 0.3784 - val_acc: 0.8856\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.2005 - acc: 0.9298 - val_loss: 0.1940 - val_acc: 0.9384\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.2622 - acc: 0.9070 - val_loss: 0.4028 - val_acc: 0.8813\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.2231 - acc: 0.9315 - val_loss: 0.3024 - val_acc: 0.9159\n",
      "(Average) Test Accuracy= 0.485354507732\n",
      "(Average) Test Loss= 1.99821751209\n",
      "\n",
      "(round: 21 )\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.2632 - acc: 0.9178 - val_loss: 0.2639 - val_acc: 0.8926\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.3088 - acc: 0.9081 - val_loss: 0.4054 - val_acc: 0.9015\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.3579 - acc: 0.8818 - val_loss: 0.5346 - val_acc: 0.8314\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.1869 - acc: 0.9360 - val_loss: 0.3700 - val_acc: 0.9039\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.3505 - acc: 0.8949 - val_loss: 0.3975 - val_acc: 0.8771\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.2285 - acc: 0.9234 - val_loss: 0.3260 - val_acc: 0.9091\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.3117 - acc: 0.8973 - val_loss: 0.6508 - val_acc: 0.8297\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.2142 - acc: 0.9324 - val_loss: 0.1327 - val_acc: 0.9692\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.4134 - acc: 0.8698 - val_loss: 0.5850 - val_acc: 0.8008\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.4050 - acc: 0.8552 - val_loss: 0.3201 - val_acc: 0.8905\n",
      "(Average) Test Accuracy= 0.481446909254\n",
      "(Average) Test Loss= 2.16817575059\n",
      "\n",
      "(round: 22 )\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.2969 - acc: 0.8998 - val_loss: 0.3354 - val_acc: 0.8841\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.3797 - acc: 0.8714 - val_loss: 0.3180 - val_acc: 0.9104\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.3320 - acc: 0.8953 - val_loss: 0.4425 - val_acc: 0.8659\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.3463 - acc: 0.8917 - val_loss: 0.3277 - val_acc: 0.8983\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.3099 - acc: 0.8874 - val_loss: 0.6276 - val_acc: 0.7991\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.2418 - acc: 0.9094 - val_loss: 0.3009 - val_acc: 0.8960\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.2173 - acc: 0.9298 - val_loss: 0.1446 - val_acc: 0.9623\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.3140 - acc: 0.8920 - val_loss: 0.3319 - val_acc: 0.8788\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1329/1329 [==============================] - 1s - loss: 0.2064 - acc: 0.9338 - val_loss: 0.3172 - val_acc: 0.9099\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.1623 - acc: 0.9430 - val_loss: 0.2792 - val_acc: 0.9217\n",
      "(Average) Test Accuracy= 0.492220919484\n",
      "(Average) Test Loss= 2.19271492339\n",
      "\n",
      "(round: 23 )\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.2180 - acc: 0.9245 - val_loss: 0.2435 - val_acc: 0.9128\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.1964 - acc: 0.9345 - val_loss: 0.2869 - val_acc: 0.9159\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.2862 - acc: 0.8981 - val_loss: 0.3531 - val_acc: 0.8771\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.2323 - acc: 0.9290 - val_loss: 0.4151 - val_acc: 0.8826\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.1632 - acc: 0.9405 - val_loss: 0.3346 - val_acc: 0.9066\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.1527 - acc: 0.9469 - val_loss: 0.1262 - val_acc: 0.9555\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.3237 - acc: 0.8886 - val_loss: 0.7469 - val_acc: 0.7701\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.3050 - acc: 0.8776 - val_loss: 0.3768 - val_acc: 0.8996\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.3448 - acc: 0.8988 - val_loss: 0.3557 - val_acc: 0.8943\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.3492 - acc: 0.8826 - val_loss: 0.3463 - val_acc: 0.8905\n",
      "(Average) Test Accuracy= 0.496140616004\n",
      "(Average) Test Loss= 2.29935297949\n",
      "\n",
      "(round: 24 )\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.2437 - acc: 0.9202 - val_loss: 0.6403 - val_acc: 0.8210\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.2115 - acc: 0.9329 - val_loss: 0.2618 - val_acc: 0.9094\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.1349 - acc: 0.9538 - val_loss: 0.3274 - val_acc: 0.9141\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.2805 - acc: 0.9076 - val_loss: 0.3049 - val_acc: 0.9005\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.2199 - acc: 0.9271 - val_loss: 0.3062 - val_acc: 0.9053\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.1414 - acc: 0.9495 - val_loss: 0.1364 - val_acc: 0.9589\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.2882 - acc: 0.9008 - val_loss: 0.3202 - val_acc: 0.8963\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.5312 - acc: 0.8525 - val_loss: 0.2850 - val_acc: 0.9069\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.3366 - acc: 0.8843 - val_loss: 0.3919 - val_acc: 0.8856\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.3136 - acc: 0.9039 - val_loss: 0.4081 - val_acc: 0.8582\n",
      "(Average) Test Accuracy= 0.509934370988\n",
      "(Average) Test Loss= 2.11872457438\n",
      "\n",
      "(round: 25 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.1569 - acc: 0.9443 - val_loss: 0.2737 - val_acc: 0.9217\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.1205 - acc: 0.9606 - val_loss: 0.1844 - val_acc: 0.9521\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.2615 - acc: 0.9135 - val_loss: 0.5514 - val_acc: 0.8276\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.2381 - acc: 0.9180 - val_loss: 0.3972 - val_acc: 0.8952\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.2623 - acc: 0.9108 - val_loss: 0.4217 - val_acc: 0.8517\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.1625 - acc: 0.9473 - val_loss: 0.3480 - val_acc: 0.9099\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.2981 - acc: 0.8978 - val_loss: 0.3313 - val_acc: 0.8882\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.3276 - acc: 0.8884 - val_loss: 0.2416 - val_acc: 0.9228\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.2656 - acc: 0.9014 - val_loss: 0.2849 - val_acc: 0.9055\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.2333 - acc: 0.9205 - val_loss: 0.3028 - val_acc: 0.9091\n",
      "(Average) Test Accuracy= 0.488582664067\n",
      "(Average) Test Loss= 2.23402792213\n",
      "\n",
      "(round: 26 )\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.1929 - acc: 0.9276 - val_loss: 0.3029 - val_acc: 0.9005\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.2350 - acc: 0.9224 - val_loss: 0.4187 - val_acc: 0.9039\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.1589 - acc: 0.9513 - val_loss: 0.3166 - val_acc: 0.8926\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.2285 - acc: 0.9232 - val_loss: 0.3527 - val_acc: 0.8760\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.3166 - acc: 0.9011 - val_loss: 0.4112 - val_acc: 0.8659\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.1449 - acc: 0.9513 - val_loss: 0.3026 - val_acc: 0.9192\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.1394 - acc: 0.9473 - val_loss: 0.4023 - val_acc: 0.8889\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.1551 - acc: 0.9536 - val_loss: 0.3037 - val_acc: 0.9167\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.1805 - acc: 0.9352 - val_loss: 0.3941 - val_acc: 0.8686\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.1124 - acc: 0.9632 - val_loss: 0.1666 - val_acc: 0.9555\n",
      "(Average) Test Accuracy= 0.501334543915\n",
      "(Average) Test Loss= 2.2170590461\n",
      "\n",
      "(round: 27 )\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.2204 - acc: 0.9202 - val_loss: 0.4068 - val_acc: 0.9039\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.2669 - acc: 0.9151 - val_loss: 0.2984 - val_acc: 0.9024\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.2686 - acc: 0.9161 - val_loss: 0.2462 - val_acc: 0.9128\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.2018 - acc: 0.9276 - val_loss: 0.3329 - val_acc: 0.9005\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.1397 - acc: 0.9503 - val_loss: 0.2775 - val_acc: 0.9189\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.1212 - acc: 0.9570 - val_loss: 0.4482 - val_acc: 0.8737\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.1188 - acc: 0.9658 - val_loss: 0.1268 - val_acc: 0.9692\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.2032 - acc: 0.9406 - val_loss: 0.3209 - val_acc: 0.8814\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.2166 - acc: 0.9337 - val_loss: 0.4667 - val_acc: 0.8582\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.2352 - acc: 0.9280 - val_loss: 0.3575 - val_acc: 0.9053\n",
      "(Average) Test Accuracy= 0.513433547471\n",
      "(Average) Test Loss= 2.18608080486\n",
      "\n",
      "(round: 28 )\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.1645 - acc: 0.9472 - val_loss: 0.4157 - val_acc: 0.8927\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.1705 - acc: 0.9355 - val_loss: 0.3643 - val_acc: 0.9258\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.1871 - acc: 0.9406 - val_loss: 0.2888 - val_acc: 0.8941\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.1108 - acc: 0.9631 - val_loss: 0.2807 - val_acc: 0.9189\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.1195 - acc: 0.9631 - val_loss: 0.3940 - val_acc: 0.8864\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.2071 - acc: 0.9295 - val_loss: 0.2975 - val_acc: 0.9094\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.1166 - acc: 0.9544 - val_loss: 0.3439 - val_acc: 0.9091\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.1120 - acc: 0.9615 - val_loss: 0.1491 - val_acc: 0.9623\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.2440 - acc: 0.9201 - val_loss: 0.2895 - val_acc: 0.9104\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.2087 - acc: 0.9339 - val_loss: 0.2978 - val_acc: 0.9085\n",
      "(Average) Test Accuracy= 0.509942951645\n",
      "(Average) Test Loss= 2.39820558292\n",
      "\n",
      "(round: 29 )\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.1093 - acc: 0.9634 - val_loss: 0.2768 - val_acc: 0.9085\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.2410 - acc: 0.9211 - val_loss: 0.2424 - val_acc: 0.9329\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.1042 - acc: 0.9632 - val_loss: 0.1367 - val_acc: 0.9658\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.1890 - acc: 0.9426 - val_loss: 0.3104 - val_acc: 0.9204\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.2121 - acc: 0.9279 - val_loss: 0.6460 - val_acc: 0.8341\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.1478 - acc: 0.9529 - val_loss: 0.3389 - val_acc: 0.8927\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.1261 - acc: 0.9594 - val_loss: 0.3215 - val_acc: 0.9129\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.2281 - acc: 0.9193 - val_loss: 0.4194 - val_acc: 0.8220\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.1378 - acc: 0.9517 - val_loss: 0.3688 - val_acc: 0.8902\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.1167 - acc: 0.9589 - val_loss: 0.2802 - val_acc: 0.9318\n",
      "(Average) Test Accuracy= 0.507309745652\n",
      "(Average) Test Loss= 2.34295480232\n",
      "\n",
      "(round: 30 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0840 - acc: 0.9728 - val_loss: 0.2753 - val_acc: 0.9192\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.1952 - acc: 0.9395 - val_loss: 0.3099 - val_acc: 0.9045\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.2658 - acc: 0.9222 - val_loss: 0.3634 - val_acc: 0.8889\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.1717 - acc: 0.9351 - val_loss: 0.3422 - val_acc: 0.8856\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.1422 - acc: 0.9555 - val_loss: 0.3209 - val_acc: 0.9167\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.1587 - acc: 0.9469 - val_loss: 0.3587 - val_acc: 0.8729\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.1116 - acc: 0.9631 - val_loss: 0.3087 - val_acc: 0.9249\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.1807 - acc: 0.9432 - val_loss: 0.3967 - val_acc: 0.9214\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.1356 - acc: 0.9522 - val_loss: 0.2524 - val_acc: 0.9128\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.1086 - acc: 0.9692 - val_loss: 0.1955 - val_acc: 0.9521\n",
      "(Average) Test Accuracy= 0.511594453663\n",
      "(Average) Test Loss= 2.35699703431\n",
      "\n",
      "(round: 31 )\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.1543 - acc: 0.9517 - val_loss: 0.3821 - val_acc: 0.9015\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0828 - acc: 0.9692 - val_loss: 0.1367 - val_acc: 0.9692\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.1219 - acc: 0.9639 - val_loss: 0.3841 - val_acc: 0.8771\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.1185 - acc: 0.9656 - val_loss: 0.2551 - val_acc: 0.9195\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0878 - acc: 0.9639 - val_loss: 0.3063 - val_acc: 0.9268\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.2209 - acc: 0.9257 - val_loss: 0.2952 - val_acc: 0.9126\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.3135 - acc: 0.9104 - val_loss: 0.4550 - val_acc: 0.8908\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.1351 - acc: 0.9529 - val_loss: 0.4341 - val_acc: 0.8621\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.1759 - acc: 0.9376 - val_loss: 0.3335 - val_acc: 0.9005\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1329/1329 [==============================] - 1s - loss: 0.1068 - acc: 0.9646 - val_loss: 0.2740 - val_acc: 0.9309\n",
      "(Average) Test Accuracy= 0.507224600139\n",
      "(Average) Test Loss= 2.3472133232\n",
      "\n",
      "(round: 32 )\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.1309 - acc: 0.9596 - val_loss: 0.4475 - val_acc: 0.9170\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.1506 - acc: 0.9480 - val_loss: 0.2618 - val_acc: 0.9110\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0919 - acc: 0.9669 - val_loss: 0.3761 - val_acc: 0.8939\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.1667 - acc: 0.9395 - val_loss: 0.3675 - val_acc: 0.8812\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0859 - acc: 0.9735 - val_loss: 0.1620 - val_acc: 0.9589\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.1554 - acc: 0.9476 - val_loss: 0.3172 - val_acc: 0.9126\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.1639 - acc: 0.9351 - val_loss: 0.3162 - val_acc: 0.8905\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.1187 - acc: 0.9614 - val_loss: 0.2687 - val_acc: 0.9128\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0991 - acc: 0.9691 - val_loss: 0.3206 - val_acc: 0.9219\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0884 - acc: 0.9690 - val_loss: 0.3373 - val_acc: 0.9066\n",
      "(Average) Test Accuracy= 0.503172350216\n",
      "(Average) Test Loss= 2.46195974818\n",
      "\n",
      "(round: 33 )\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.1129 - acc: 0.9654 - val_loss: 0.4597 - val_acc: 0.8621\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0644 - acc: 0.9785 - val_loss: 0.3369 - val_acc: 0.9116\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.1385 - acc: 0.9565 - val_loss: 0.3404 - val_acc: 0.8517\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0738 - acc: 0.9737 - val_loss: 0.3431 - val_acc: 0.9249\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.1619 - acc: 0.9491 - val_loss: 0.2889 - val_acc: 0.9146\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.1092 - acc: 0.9692 - val_loss: 0.1361 - val_acc: 0.9658\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.1014 - acc: 0.9678 - val_loss: 0.4502 - val_acc: 0.8939\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.1764 - acc: 0.9475 - val_loss: 0.4036 - val_acc: 0.9083\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.1014 - acc: 0.9706 - val_loss: 0.2764 - val_acc: 0.9262\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.1199 - acc: 0.9551 - val_loss: 0.3202 - val_acc: 0.8756\n",
      "(Average) Test Accuracy= 0.501102104018\n",
      "(Average) Test Loss= 2.50029104265\n",
      "\n",
      "(round: 34 )\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0809 - acc: 0.9669 - val_loss: 0.3431 - val_acc: 0.9309\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0977 - acc: 0.9693 - val_loss: 0.4113 - val_acc: 0.8736\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.1131 - acc: 0.9659 - val_loss: 0.4592 - val_acc: 0.8902\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.1226 - acc: 0.9618 - val_loss: 0.4016 - val_acc: 0.8856\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0999 - acc: 0.9627 - val_loss: 0.2645 - val_acc: 0.9369\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.1111 - acc: 0.9585 - val_loss: 0.4059 - val_acc: 0.9258\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0745 - acc: 0.9777 - val_loss: 0.1630 - val_acc: 0.9589\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.1540 - acc: 0.9501 - val_loss: 0.3544 - val_acc: 0.8706\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.1046 - acc: 0.9622 - val_loss: 0.2805 - val_acc: 0.9262\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.1510 - acc: 0.9446 - val_loss: 0.2731 - val_acc: 0.9167\n",
      "(Average) Test Accuracy= 0.523886796091\n",
      "(Average) Test Loss= 2.46375313943\n",
      "\n",
      "(round: 35 )\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.1080 - acc: 0.9692 - val_loss: 0.1539 - val_acc: 0.9658\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.1022 - acc: 0.9671 - val_loss: 0.3217 - val_acc: 0.8983\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.1464 - acc: 0.9475 - val_loss: 0.4414 - val_acc: 0.9083\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.1007 - acc: 0.9683 - val_loss: 0.7858 - val_acc: 0.8084\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.1550 - acc: 0.9488 - val_loss: 0.2900 - val_acc: 0.8955\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0890 - acc: 0.9652 - val_loss: 0.2709 - val_acc: 0.9293\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0882 - acc: 0.9754 - val_loss: 0.3570 - val_acc: 0.9053\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.1379 - acc: 0.9542 - val_loss: 0.2547 - val_acc: 0.9390\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.1238 - acc: 0.9589 - val_loss: 0.2484 - val_acc: 0.9362\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0949 - acc: 0.9676 - val_loss: 0.3097 - val_acc: 0.9369\n",
      "(Average) Test Accuracy= 0.519207266128\n",
      "(Average) Test Loss= 2.54435469741\n",
      "\n",
      "(round: 36 )\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.1175 - acc: 0.9603 - val_loss: 0.2709 - val_acc: 0.9248\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0954 - acc: 0.9675 - val_loss: 0.1404 - val_acc: 0.9623\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.1046 - acc: 0.9638 - val_loss: 0.3399 - val_acc: 0.9055\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.1332 - acc: 0.9432 - val_loss: 0.6124 - val_acc: 0.8690\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0774 - acc: 0.9696 - val_loss: 0.3506 - val_acc: 0.9167\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0794 - acc: 0.9790 - val_loss: 0.2704 - val_acc: 0.9161\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0793 - acc: 0.9699 - val_loss: 0.3145 - val_acc: 0.9249\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.1266 - acc: 0.9607 - val_loss: 0.3221 - val_acc: 0.8983\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.1215 - acc: 0.9577 - val_loss: 0.4282 - val_acc: 0.8851\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.1314 - acc: 0.9564 - val_loss: 0.4001 - val_acc: 0.9053\n",
      "(Average) Test Accuracy= 0.509704076072\n",
      "(Average) Test Loss= 2.52353246704\n",
      "\n",
      "(round: 37 )\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0788 - acc: 0.9765 - val_loss: 0.2691 - val_acc: 0.9295\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0820 - acc: 0.9712 - val_loss: 0.3812 - val_acc: 0.9004\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0732 - acc: 0.9715 - val_loss: 0.3800 - val_acc: 0.9167\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0906 - acc: 0.9703 - val_loss: 0.3269 - val_acc: 0.8856\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0490 - acc: 0.9854 - val_loss: 0.1326 - val_acc: 0.9623\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0753 - acc: 0.9744 - val_loss: 0.3474 - val_acc: 0.9279\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.1050 - acc: 0.9625 - val_loss: 0.3459 - val_acc: 0.8806\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.1264 - acc: 0.9585 - val_loss: 0.4806 - val_acc: 0.8996\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0800 - acc: 0.9735 - val_loss: 0.3776 - val_acc: 0.9129\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.1298 - acc: 0.9573 - val_loss: 0.3194 - val_acc: 0.9228\n",
      "(Average) Test Accuracy= 0.508482870273\n",
      "(Average) Test Loss= 2.63496289816\n",
      "\n",
      "(round: 38 )\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.1843 - acc: 0.9436 - val_loss: 0.2979 - val_acc: 0.9339\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.1142 - acc: 0.9650 - val_loss: 0.2835 - val_acc: 0.8898\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0324 - acc: 0.9906 - val_loss: 0.1448 - val_acc: 0.9692\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0640 - acc: 0.9782 - val_loss: 0.3869 - val_acc: 0.9091\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0802 - acc: 0.9713 - val_loss: 0.3853 - val_acc: 0.8905\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.1014 - acc: 0.9674 - val_loss: 0.2566 - val_acc: 0.9289\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.2208 - acc: 0.9322 - val_loss: 0.4806 - val_acc: 0.8908\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0752 - acc: 0.9790 - val_loss: 0.2568 - val_acc: 0.9262\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0705 - acc: 0.9722 - val_loss: 0.3859 - val_acc: 0.9040\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0787 - acc: 0.9779 - val_loss: 0.3944 - val_acc: 0.8927\n",
      "(Average) Test Accuracy= 0.515865674272\n",
      "(Average) Test Loss= 2.47167547326\n",
      "\n",
      "(round: 39 )\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0615 - acc: 0.9792 - val_loss: 0.3952 - val_acc: 0.9091\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0531 - acc: 0.9829 - val_loss: 0.3295 - val_acc: 0.9141\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0895 - acc: 0.9682 - val_loss: 0.2899 - val_acc: 0.9110\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0950 - acc: 0.9700 - val_loss: 0.3452 - val_acc: 0.9104\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0599 - acc: 0.9812 - val_loss: 0.3381 - val_acc: 0.9309\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.1003 - acc: 0.9645 - val_loss: 0.4451 - val_acc: 0.8736\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.1306 - acc: 0.9552 - val_loss: 0.2792 - val_acc: 0.9268\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.1734 - acc: 0.9388 - val_loss: 0.5429 - val_acc: 0.8777\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0739 - acc: 0.9740 - val_loss: 0.2746 - val_acc: 0.9195\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0597 - acc: 0.9803 - val_loss: 0.1600 - val_acc: 0.9692\n",
      "(Average) Test Accuracy= 0.517386361415\n",
      "(Average) Test Loss= 2.58033099814\n",
      "\n",
      "(round: 40 )\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0689 - acc: 0.9798 - val_loss: 0.3247 - val_acc: 0.8771\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0261 - acc: 0.9940 - val_loss: 0.1471 - val_acc: 0.9726\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0465 - acc: 0.9830 - val_loss: 0.4076 - val_acc: 0.9053\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0596 - acc: 0.9790 - val_loss: 0.2633 - val_acc: 0.9295\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0952 - acc: 0.9736 - val_loss: 0.2647 - val_acc: 0.9207\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.1437 - acc: 0.9549 - val_loss: 0.4204 - val_acc: 0.8889\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0740 - acc: 0.9759 - val_loss: 0.3954 - val_acc: 0.9066\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.1194 - acc: 0.9613 - val_loss: 0.4025 - val_acc: 0.8905\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.1007 - acc: 0.9650 - val_loss: 0.4547 - val_acc: 0.9127\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0570 - acc: 0.9789 - val_loss: 0.3317 - val_acc: 0.9279\n",
      "(Average) Test Accuracy= 0.514206000061\n",
      "(Average) Test Loss= 2.64617906676\n",
      "\n",
      "(round: 41 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0438 - acc: 0.9857 - val_loss: 0.2837 - val_acc: 0.9195\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0904 - acc: 0.9713 - val_loss: 0.3960 - val_acc: 0.9104\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0768 - acc: 0.9735 - val_loss: 0.3568 - val_acc: 0.9091\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0903 - acc: 0.9731 - val_loss: 0.4130 - val_acc: 0.8927\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0486 - acc: 0.9820 - val_loss: 0.1629 - val_acc: 0.9658\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0733 - acc: 0.9788 - val_loss: 0.2850 - val_acc: 0.9068\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0616 - acc: 0.9804 - val_loss: 0.3049 - val_acc: 0.9217\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0400 - acc: 0.9887 - val_loss: 0.3298 - val_acc: 0.9369\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0877 - acc: 0.9716 - val_loss: 0.5466 - val_acc: 0.9083\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.1257 - acc: 0.9639 - val_loss: 0.2454 - val_acc: 0.9309\n",
      "(Average) Test Accuracy= 0.526847958653\n",
      "(Average) Test Loss= 2.60180429002\n",
      "\n",
      "(round: 42 )\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.2336 - acc: 0.9257 - val_loss: 0.2235 - val_acc: 0.9237\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0537 - acc: 0.9850 - val_loss: 0.3169 - val_acc: 0.9309\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0714 - acc: 0.9788 - val_loss: 0.3316 - val_acc: 0.9055\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0436 - acc: 0.9848 - val_loss: 0.3947 - val_acc: 0.8977\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0623 - acc: 0.9807 - val_loss: 0.2914 - val_acc: 0.9060\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0691 - acc: 0.9814 - val_loss: 0.4662 - val_acc: 0.9170\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0495 - acc: 0.9856 - val_loss: 0.4596 - val_acc: 0.8812\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.1021 - acc: 0.9624 - val_loss: 0.2691 - val_acc: 0.9167\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0984 - acc: 0.9709 - val_loss: 0.2499 - val_acc: 0.9419\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0509 - acc: 0.9846 - val_loss: 0.1888 - val_acc: 0.9555\n",
      "(Average) Test Accuracy= 0.536228965236\n",
      "(Average) Test Loss= 2.48381233982\n",
      "\n",
      "(round: 43 )\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0650 - acc: 0.9754 - val_loss: 0.4170 - val_acc: 0.9091\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0685 - acc: 0.9827 - val_loss: 0.4075 - val_acc: 0.8889\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0806 - acc: 0.9766 - val_loss: 0.2879 - val_acc: 0.9126\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0712 - acc: 0.9782 - val_loss: 0.2658 - val_acc: 0.9195\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0599 - acc: 0.9837 - val_loss: 0.1491 - val_acc: 0.9692\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0629 - acc: 0.9798 - val_loss: 0.2981 - val_acc: 0.9237\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0445 - acc: 0.9842 - val_loss: 0.2812 - val_acc: 0.9242\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0844 - acc: 0.9750 - val_loss: 0.3685 - val_acc: 0.9055\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0623 - acc: 0.9781 - val_loss: 0.5005 - val_acc: 0.9083\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0516 - acc: 0.9782 - val_loss: 0.3052 - val_acc: 0.9369\n",
      "(Average) Test Accuracy= 0.514969154954\n",
      "(Average) Test Loss= 2.68156420115\n",
      "\n",
      "(round: 44 )\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0445 - acc: 0.9872 - val_loss: 0.3247 - val_acc: 0.9339\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0342 - acc: 0.9886 - val_loss: 0.3006 - val_acc: 0.9293\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0333 - acc: 0.9914 - val_loss: 0.1469 - val_acc: 0.9658\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0512 - acc: 0.9820 - val_loss: 0.4738 - val_acc: 0.9129\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0620 - acc: 0.9827 - val_loss: 0.4301 - val_acc: 0.8851\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0530 - acc: 0.9847 - val_loss: 0.4777 - val_acc: 0.9127\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0853 - acc: 0.9725 - val_loss: 0.3743 - val_acc: 0.8905\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0627 - acc: 0.9786 - val_loss: 0.2901 - val_acc: 0.9167\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.1418 - acc: 0.9618 - val_loss: 0.3109 - val_acc: 0.8941\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0601 - acc: 0.9790 - val_loss: 0.2968 - val_acc: 0.9329\n",
      "(Average) Test Accuracy= 0.518347750149\n",
      "(Average) Test Loss= 2.80064125114\n",
      "\n",
      "(round: 45 )\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0687 - acc: 0.9781 - val_loss: 0.5034 - val_acc: 0.8996\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0720 - acc: 0.9700 - val_loss: 0.3863 - val_acc: 0.9005\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0329 - acc: 0.9899 - val_loss: 0.2946 - val_acc: 0.9362\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0527 - acc: 0.9816 - val_loss: 0.2900 - val_acc: 0.9242\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0283 - acc: 0.9895 - val_loss: 0.3464 - val_acc: 0.9399\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0768 - acc: 0.9692 - val_loss: 0.3045 - val_acc: 0.9025\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 1s - loss: 0.0212 - acc: 0.9949 - val_loss: 0.1592 - val_acc: 0.9692\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0912 - acc: 0.9720 - val_loss: 0.2958 - val_acc: 0.9167\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.1025 - acc: 0.9716 - val_loss: 0.4161 - val_acc: 0.9053\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0723 - acc: 0.9779 - val_loss: 0.4742 - val_acc: 0.8697\n",
      "(Average) Test Accuracy= 0.526293501549\n",
      "(Average) Test Loss= 2.76415612778\n",
      "\n",
      "(round: 46 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0499 - acc: 0.9835 - val_loss: 0.2822 - val_acc: 0.9343\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.1053 - acc: 0.9725 - val_loss: 0.2777 - val_acc: 0.9268\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0989 - acc: 0.9628 - val_loss: 0.3229 - val_acc: 0.8983\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0551 - acc: 0.9839 - val_loss: 0.4228 - val_acc: 0.9205\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0204 - acc: 0.9966 - val_loss: 0.1715 - val_acc: 0.9623\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0586 - acc: 0.9808 - val_loss: 0.4503 - val_acc: 0.8812\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0544 - acc: 0.9832 - val_loss: 0.2617 - val_acc: 0.9195\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0734 - acc: 0.9788 - val_loss: 0.3734 - val_acc: 0.9104\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0449 - acc: 0.9842 - val_loss: 0.3429 - val_acc: 0.9339\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0617 - acc: 0.9814 - val_loss: 0.5286 - val_acc: 0.8908\n",
      "(Average) Test Accuracy= 0.514553441025\n",
      "(Average) Test Loss= 2.68475342489\n",
      "\n",
      "(round: 47 )\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0554 - acc: 0.9830 - val_loss: 0.2968 - val_acc: 0.8983\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0343 - acc: 0.9887 - val_loss: 0.3697 - val_acc: 0.9189\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0496 - acc: 0.9813 - val_loss: 0.3503 - val_acc: 0.8955\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0260 - acc: 0.9914 - val_loss: 0.1833 - val_acc: 0.9589\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0572 - acc: 0.9792 - val_loss: 0.5274 - val_acc: 0.9039\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0374 - acc: 0.9858 - val_loss: 0.4160 - val_acc: 0.9242\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0805 - acc: 0.9786 - val_loss: 0.3025 - val_acc: 0.9167\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0663 - acc: 0.9782 - val_loss: 0.3205 - val_acc: 0.9128\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0523 - acc: 0.9823 - val_loss: 0.3043 - val_acc: 0.9369\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0430 - acc: 0.9856 - val_loss: 0.5464 - val_acc: 0.8927\n",
      "(Average) Test Accuracy= 0.521243624074\n",
      "(Average) Test Loss= 2.83454101463\n",
      "\n",
      "(round: 48 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0281 - acc: 0.9899 - val_loss: 0.2752 - val_acc: 0.9369\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0469 - acc: 0.9846 - val_loss: 0.4778 - val_acc: 0.8851\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0408 - acc: 0.9889 - val_loss: 0.1836 - val_acc: 0.9623\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0693 - acc: 0.9807 - val_loss: 0.2922 - val_acc: 0.9187\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0815 - acc: 0.9752 - val_loss: 0.3415 - val_acc: 0.9309\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0518 - acc: 0.9836 - val_loss: 0.6362 - val_acc: 0.8821\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0511 - acc: 0.9851 - val_loss: 0.3106 - val_acc: 0.9068\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0418 - acc: 0.9896 - val_loss: 0.4915 - val_acc: 0.9091\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0444 - acc: 0.9850 - val_loss: 0.3567 - val_acc: 0.8856\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0628 - acc: 0.9824 - val_loss: 0.2811 - val_acc: 0.9295\n",
      "(Average) Test Accuracy= 0.52644106442\n",
      "(Average) Test Loss= 2.8122196169\n",
      "\n",
      "(round: 49 )\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0330 - acc: 0.9923 - val_loss: 0.1857 - val_acc: 0.9658\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0287 - acc: 0.9896 - val_loss: 0.4754 - val_acc: 0.8939\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0189 - acc: 0.9955 - val_loss: 0.3581 - val_acc: 0.9279\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0510 - acc: 0.9825 - val_loss: 0.3761 - val_acc: 0.9055\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0329 - acc: 0.9924 - val_loss: 0.2966 - val_acc: 0.9262\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0530 - acc: 0.9825 - val_loss: 0.5832 - val_acc: 0.9083\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0624 - acc: 0.9779 - val_loss: 0.4046 - val_acc: 0.9080\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0810 - acc: 0.9736 - val_loss: 0.2862 - val_acc: 0.9207\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.1176 - acc: 0.9660 - val_loss: 0.2927 - val_acc: 0.9025\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0383 - acc: 0.9848 - val_loss: 0.3301 - val_acc: 0.9116\n",
      "(Average) Test Accuracy= 0.522104538654\n",
      "(Average) Test Loss= 2.83462666654\n",
      "\n",
      "(round: 50 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0307 - acc: 0.9886 - val_loss: 0.3060 - val_acc: 0.9268\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0473 - acc: 0.9894 - val_loss: 0.4047 - val_acc: 0.9080\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0331 - acc: 0.9880 - val_loss: 0.3536 - val_acc: 0.9339\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0782 - acc: 0.9745 - val_loss: 0.3250 - val_acc: 0.8898\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0459 - acc: 0.9866 - val_loss: 0.2443 - val_acc: 0.9295\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0632 - acc: 0.9822 - val_loss: 0.3069 - val_acc: 0.9248\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.1065 - acc: 0.9639 - val_loss: 0.5572 - val_acc: 0.9214\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0646 - acc: 0.9820 - val_loss: 0.4634 - val_acc: 0.9205\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0299 - acc: 0.9897 - val_loss: 0.1910 - val_acc: 0.9555\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0647 - acc: 0.9738 - val_loss: 0.3754 - val_acc: 0.9055\n",
      "(Average) Test Accuracy= 0.529194885684\n",
      "(Average) Test Loss= 2.80133663258\n",
      "\n",
      "(round: 51 )\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0279 - acc: 0.9925 - val_loss: 0.3254 - val_acc: 0.9309\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0517 - acc: 0.9875 - val_loss: 0.3883 - val_acc: 0.8806\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0649 - acc: 0.9807 - val_loss: 0.3134 - val_acc: 0.9228\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0730 - acc: 0.9759 - val_loss: 0.2945 - val_acc: 0.9343\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0442 - acc: 0.9894 - val_loss: 0.4939 - val_acc: 0.8889\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0609 - acc: 0.9841 - val_loss: 0.2997 - val_acc: 0.8941\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0496 - acc: 0.9792 - val_loss: 0.5219 - val_acc: 0.9083\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0264 - acc: 0.9906 - val_loss: 0.1837 - val_acc: 0.9623\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0322 - acc: 0.9891 - val_loss: 0.2425 - val_acc: 0.9295\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0398 - acc: 0.9896 - val_loss: 0.4523 - val_acc: 0.9091\n",
      "(Average) Test Accuracy= 0.531834480232\n",
      "(Average) Test Loss= 2.70542294739\n",
      "\n",
      "(round: 52 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0322 - acc: 0.9911 - val_loss: 0.2885 - val_acc: 0.9293\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0327 - acc: 0.9880 - val_loss: 0.3881 - val_acc: 0.9279\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0207 - acc: 0.9934 - val_loss: 0.4729 - val_acc: 0.9091\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0281 - acc: 0.9916 - val_loss: 0.2285 - val_acc: 0.9329\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0336 - acc: 0.9904 - val_loss: 0.4890 - val_acc: 0.8927\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0828 - acc: 0.9786 - val_loss: 0.3023 - val_acc: 0.9268\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0911 - acc: 0.9650 - val_loss: 0.4137 - val_acc: 0.9154\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0667 - acc: 0.9792 - val_loss: 0.5661 - val_acc: 0.8996\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0284 - acc: 0.9936 - val_loss: 0.2922 - val_acc: 0.9068\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0193 - acc: 0.9923 - val_loss: 0.1907 - val_acc: 0.9692\n",
      "(Average) Test Accuracy= 0.524664575377\n",
      "(Average) Test Loss= 2.87065803695\n",
      "\n",
      "(round: 53 )\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0415 - acc: 0.9866 - val_loss: 0.2595 - val_acc: 0.9228\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0395 - acc: 0.9825 - val_loss: 0.5251 - val_acc: 0.9258\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0265 - acc: 0.9933 - val_loss: 0.4719 - val_acc: 0.8812\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0373 - acc: 0.9883 - val_loss: 0.3629 - val_acc: 0.8771\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0566 - acc: 0.9867 - val_loss: 0.3204 - val_acc: 0.9495\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0384 - acc: 0.9872 - val_loss: 0.3476 - val_acc: 0.9309\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0493 - acc: 0.9850 - val_loss: 0.4144 - val_acc: 0.8955\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0225 - acc: 0.9949 - val_loss: 0.2067 - val_acc: 0.9658\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0800 - acc: 0.9771 - val_loss: 0.3098 - val_acc: 0.9207\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0850 - acc: 0.9697 - val_loss: 0.4864 - val_acc: 0.9053\n",
      "(Average) Test Accuracy= 0.516515107687\n",
      "(Average) Test Loss= 2.85882340816\n",
      "\n",
      "(round: 54 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0355 - acc: 0.9873 - val_loss: 0.3281 - val_acc: 0.9343\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0715 - acc: 0.9781 - val_loss: 0.5848 - val_acc: 0.9039\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0636 - acc: 0.9750 - val_loss: 0.4456 - val_acc: 0.9005\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0211 - acc: 0.9949 - val_loss: 0.2036 - val_acc: 0.9521\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0430 - acc: 0.9857 - val_loss: 0.2310 - val_acc: 0.9262\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0786 - acc: 0.9777 - val_loss: 0.2630 - val_acc: 0.8898\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0526 - acc: 0.9852 - val_loss: 0.2914 - val_acc: 0.9329\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1329/1329 [==============================] - 1s - loss: 0.0536 - acc: 0.9842 - val_loss: 0.3615 - val_acc: 0.9279\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0438 - acc: 0.9798 - val_loss: 0.5288 - val_acc: 0.8736\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0451 - acc: 0.9877 - val_loss: 0.4814 - val_acc: 0.9053\n",
      "(Average) Test Accuracy= 0.535406538461\n",
      "(Average) Test Loss= 2.78417084256\n",
      "\n",
      "(round: 55 )\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0350 - acc: 0.9851 - val_loss: 0.2543 - val_acc: 0.9025\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0518 - acc: 0.9813 - val_loss: 0.4412 - val_acc: 0.9005\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0314 - acc: 0.9867 - val_loss: 0.4837 - val_acc: 0.9015\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0333 - acc: 0.9887 - val_loss: 0.3614 - val_acc: 0.9339\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0249 - acc: 0.9923 - val_loss: 0.1550 - val_acc: 0.9623\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0535 - acc: 0.9880 - val_loss: 0.5891 - val_acc: 0.9127\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0394 - acc: 0.9873 - val_loss: 0.2741 - val_acc: 0.9419\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0307 - acc: 0.9914 - val_loss: 0.4335 - val_acc: 0.9042\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0626 - acc: 0.9802 - val_loss: 0.3031 - val_acc: 0.9207\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0674 - acc: 0.9765 - val_loss: 0.2509 - val_acc: 0.9329\n",
      "(Average) Test Accuracy= 0.528071389345\n",
      "(Average) Test Loss= 2.80807712621\n",
      "\n",
      "(round: 56 )\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0273 - acc: 0.9902 - val_loss: 0.3637 - val_acc: 0.9339\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0604 - acc: 0.9807 - val_loss: 0.2863 - val_acc: 0.9268\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.1012 - acc: 0.9639 - val_loss: 0.4980 - val_acc: 0.8996\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0374 - acc: 0.9867 - val_loss: 0.4628 - val_acc: 0.9167\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0755 - acc: 0.9813 - val_loss: 0.3852 - val_acc: 0.8955\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0323 - acc: 0.9883 - val_loss: 0.2638 - val_acc: 0.9329\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0358 - acc: 0.9924 - val_loss: 0.2661 - val_acc: 0.9444\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0367 - acc: 0.9894 - val_loss: 0.4547 - val_acc: 0.8851\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0172 - acc: 0.9957 - val_loss: 0.1765 - val_acc: 0.9658\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0227 - acc: 0.9936 - val_loss: 0.2975 - val_acc: 0.8898\n",
      "(Average) Test Accuracy= 0.528604473782\n",
      "(Average) Test Loss= 2.81689064675\n",
      "\n",
      "(round: 57 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0251 - acc: 0.9937 - val_loss: 0.2699 - val_acc: 0.9394\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0240 - acc: 0.9953 - val_loss: 0.4883 - val_acc: 0.9167\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0219 - acc: 0.9947 - val_loss: 0.3151 - val_acc: 0.8941\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0232 - acc: 0.9925 - val_loss: 0.3791 - val_acc: 0.9309\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0456 - acc: 0.9852 - val_loss: 0.3012 - val_acc: 0.9207\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0328 - acc: 0.9863 - val_loss: 0.1711 - val_acc: 0.9658\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0316 - acc: 0.9913 - val_loss: 0.6036 - val_acc: 0.8996\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0429 - acc: 0.9863 - val_loss: 0.4720 - val_acc: 0.9204\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0219 - acc: 0.9933 - val_loss: 0.5261 - val_acc: 0.8889\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0172 - acc: 0.9933 - val_loss: 0.2614 - val_acc: 0.9329\n",
      "(Average) Test Accuracy= 0.524629691248\n",
      "(Average) Test Loss= 2.98689243182\n",
      "\n",
      "(round: 58 )\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0392 - acc: 0.9873 - val_loss: 0.3239 - val_acc: 0.8983\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0189 - acc: 0.9924 - val_loss: 0.2809 - val_acc: 0.9195\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0525 - acc: 0.9837 - val_loss: 0.3342 - val_acc: 0.9187\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0450 - acc: 0.9888 - val_loss: 0.4917 - val_acc: 0.9055\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0181 - acc: 0.9949 - val_loss: 0.1989 - val_acc: 0.9623\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0263 - acc: 0.9899 - val_loss: 0.3227 - val_acc: 0.9419\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0275 - acc: 0.9910 - val_loss: 0.4147 - val_acc: 0.9309\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0282 - acc: 0.9914 - val_loss: 0.4939 - val_acc: 0.8927\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0243 - acc: 0.9943 - val_loss: 0.5419 - val_acc: 0.8977\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0396 - acc: 0.9858 - val_loss: 0.5851 - val_acc: 0.8952\n",
      "(Average) Test Accuracy= 0.517416686415\n",
      "(Average) Test Loss= 3.0660562726\n",
      "\n",
      "(round: 59 )\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0243 - acc: 0.9933 - val_loss: 0.4377 - val_acc: 0.9004\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0411 - acc: 0.9863 - val_loss: 0.3247 - val_acc: 0.9248\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0443 - acc: 0.9877 - val_loss: 0.5102 - val_acc: 0.8939\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0255 - acc: 0.9932 - val_loss: 0.4462 - val_acc: 0.9279\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0504 - acc: 0.9825 - val_loss: 0.4741 - val_acc: 0.8905\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0315 - acc: 0.9918 - val_loss: 0.2990 - val_acc: 0.9470\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0150 - acc: 0.9949 - val_loss: 0.2106 - val_acc: 0.9692\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0227 - acc: 0.9915 - val_loss: 0.3086 - val_acc: 0.9153\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0484 - acc: 0.9841 - val_loss: 0.3494 - val_acc: 0.9161\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0414 - acc: 0.9858 - val_loss: 0.5943 - val_acc: 0.8996\n",
      "(Average) Test Accuracy= 0.524028144888\n",
      "(Average) Test Loss= 2.97013285778\n",
      "\n",
      "It cost 1952.332604 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tStart = time.time()#計時開始\n",
    "\n",
    "#for i in range(0,10,1):\n",
    "epoch_num=60\n",
    "for j in range(0,epoch_num,1):\n",
    "    testarr=numpy.arange(0, 10, 1)\n",
    "    random.shuffle(testarr)\n",
    "    \n",
    "    arr_accuracy=np.zeros(10)\n",
    "    arr_loss=np.zeros(10)\n",
    "    arr_num=np.zeros(10)\n",
    "    print(\"(round:\",j,\")\")\n",
    "    for ii in range(0,10,1):\n",
    "        i=testarr[ii]\n",
    "        \n",
    "        Lmystr=\"NewLtest\"+str(i)+\".npy\"\n",
    "        Rmystr=\"NewRtest\"+str(i)+\".npy\"\n",
    "        LX_test=np.load(Lmystr)\n",
    "        RX_test=np.load(Rmystr)\n",
    "        x_img_test=np.concatenate((LX_test, RX_test), axis=0)\n",
    "\n",
    "        Lmystr2=\"NewLtrain\"+str(i)+\".npy\"\n",
    "        Rmystr2=\"NewRtrain\"+str(i)+\".npy\"\n",
    "        LX_train=np.load(Lmystr2)\n",
    "        RX_train=np.load(Rmystr2)\n",
    "        x_img_train=np.concatenate((LX_train, RX_train), axis=0)\n",
    "\n",
    "        Ltempx=np.load(Lobj_label_DATA_DIR[i])    \n",
    "        Rtempx=np.load(Robj_label_DATA_DIR[i])\n",
    "        y_label_train=np.concatenate((Ltempx, Rtempx), axis=0)\n",
    "\n",
    "        TLtempx=np.load(TLobj_label_DATA_DIR[i])    \n",
    "        TRtempx=np.load(TRobj_label_DATA_DIR[i])\n",
    "        y_label_test=np.concatenate((TLtempx, TRtempx), axis=0)\n",
    "\n",
    "        #Shuffle\n",
    "        x_img_train, y_label_train = shuffle(x_img_train, y_label_train, random_state=0)\n",
    "        x_img_test, y_label_test = shuffle(x_img_test, y_label_test, random_state=0)\n",
    "\n",
    "        #Image normalize\n",
    "        x_img_train_normalize = x_img_train.astype('float32') / 255.0\n",
    "        x_img_test_normalize = x_img_test.astype('float32') / 255.0\n",
    "\n",
    "        #轉換label 為OneHot Encoding\n",
    "        y_label_train_OneHot = np_utils.to_categorical(y_label_train, num_classes=24)\n",
    "        y_label_test_OneHot = np_utils.to_categorical(y_label_test, num_classes=24)\n",
    "\n",
    "        #print: train data/test data\n",
    "        #print(\"(round:\",j,\", dataset:\",i,\")\")\n",
    "        #print(\"train data:\",'images:',x_img_train.shape,\n",
    "        #      \" labels:\",y_label_train.shape) \n",
    "        #print(\"test  data:\",'images:',x_img_test.shape ,\n",
    "        #      \" labels:\",y_label_test.shape)\n",
    "\n",
    "        train_history=model.fit(x_img_train_normalize, y_label_train_OneHot, \n",
    "                                validation_split=0.2, epochs=1, \n",
    "                                batch_size=64, verbose=1)\n",
    "        \n",
    "        save_train_history(train_history,'acc','val_acc')\n",
    "        save_train_history2(train_history,'loss','val_loss')\n",
    "               \n",
    "        score = model.evaluate(x_img_test_normalize, y_label_test_OneHot, verbose=0)\n",
    "        #arr_num[i]=x_img_train_normalize.shape[0]       \n",
    "        \n",
    "        arr_num[i]=x_img_train_normalize.shape[0]       \n",
    "        arr_accuracy[i]=score[1]\n",
    "        arr_loss[i]=score[0]\n",
    "    \n",
    "    average_accuracy=np.average(arr_accuracy, weights=arr_num)\n",
    "    print(\"(Average) Test Accuracy=\",average_accuracy)\n",
    "\n",
    "    average_loss=np.average(arr_loss, weights=arr_num)\n",
    "    print(\"(Average) Test Loss=\",average_loss)\n",
    "    print(\"\")\n",
    "    testaccList.append(average_accuracy)\n",
    "    teslossList.append(average_loss)\n",
    "        \n",
    "        #score = model.evaluate(x_img_train_normalize, y_label_train_OneHot, verbose=0)\n",
    "        #print('Test loss:', score[0])\n",
    "        #print('Test accuracy:', score[1])\n",
    "        \n",
    "        #arr_num[i]=x_img_train_normalize.shape[0]       \n",
    "        #arr_accuracy[i]=score[1]\n",
    "        #arr_loss[i]=score[0]\n",
    "tEnd = time.time() #計時結束\n",
    "print (\"It cost %f sec\" % (tEnd - tStart))#會自動做近位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(round: 0 )\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0467 - acc: 0.9873 - val_loss: 0.3159 - val_acc: 0.8898\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0147 - acc: 0.9955 - val_loss: 0.3925 - val_acc: 0.9339\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0299 - acc: 0.9894 - val_loss: 0.4504 - val_acc: 0.8966\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0598 - acc: 0.9825 - val_loss: 0.4863 - val_acc: 0.8905\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0148 - acc: 0.9953 - val_loss: 0.5098 - val_acc: 0.9129\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0315 - acc: 0.9913 - val_loss: 0.5883 - val_acc: 0.9083\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0454 - acc: 0.9863 - val_loss: 0.3260 - val_acc: 0.9187\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0191 - acc: 0.9957 - val_loss: 0.2034 - val_acc: 0.9692\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0285 - acc: 0.9924 - val_loss: 0.2992 - val_acc: 0.9419\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0287 - acc: 0.9924 - val_loss: 0.2906 - val_acc: 0.9329\n",
      "(Average) Test Accuracy= 0.522567549652\n",
      "(Average) Test Loss= 3.01699391051\n",
      "\n",
      "(round: 1 )\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0240 - acc: 0.9958 - val_loss: 0.2953 - val_acc: 0.8983\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0280 - acc: 0.9915 - val_loss: 0.4833 - val_acc: 0.9167\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0321 - acc: 0.9880 - val_loss: 0.6190 - val_acc: 0.9170\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0127 - acc: 0.9940 - val_loss: 0.2724 - val_acc: 0.9486\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0331 - acc: 0.9874 - val_loss: 0.2472 - val_acc: 0.9329\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0233 - acc: 0.9923 - val_loss: 0.4157 - val_acc: 0.9080\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0198 - acc: 0.9962 - val_loss: 0.3187 - val_acc: 0.9318\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0497 - acc: 0.9847 - val_loss: 0.3154 - val_acc: 0.9329\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0410 - acc: 0.9865 - val_loss: 0.3899 - val_acc: 0.9309\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0302 - acc: 0.9900 - val_loss: 0.5451 - val_acc: 0.8955\n",
      "(Average) Test Accuracy= 0.526862716427\n",
      "(Average) Test Loss= 3.10437388882\n",
      "\n",
      "(round: 2 )\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0356 - acc: 0.9863 - val_loss: 0.4965 - val_acc: 0.9005\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0324 - acc: 0.9877 - val_loss: 0.5305 - val_acc: 0.9091\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0149 - acc: 0.9968 - val_loss: 0.3256 - val_acc: 0.8941\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0229 - acc: 0.9923 - val_loss: 0.5494 - val_acc: 0.8851\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0160 - acc: 0.9975 - val_loss: 0.2467 - val_acc: 0.9295\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0470 - acc: 0.9888 - val_loss: 0.3138 - val_acc: 0.9248\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0375 - acc: 0.9880 - val_loss: 0.3241 - val_acc: 0.9268\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0106 - acc: 0.9970 - val_loss: 0.3865 - val_acc: 0.9189\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0446 - acc: 0.9891 - val_loss: 0.5728 - val_acc: 0.9039\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0174 - acc: 0.9949 - val_loss: 0.2279 - val_acc: 0.9623\n",
      "(Average) Test Accuracy= 0.532543531426\n",
      "(Average) Test Loss= 3.03750573587\n",
      "\n",
      "(round: 3 )\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0269 - acc: 0.9904 - val_loss: 0.4869 - val_acc: 0.8927\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0318 - acc: 0.9902 - val_loss: 0.6242 - val_acc: 0.9127\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0395 - acc: 0.9858 - val_loss: 0.5822 - val_acc: 0.9015\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0492 - acc: 0.9875 - val_loss: 0.4583 - val_acc: 0.9005\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0333 - acc: 0.9888 - val_loss: 0.2990 - val_acc: 0.9187\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0585 - acc: 0.9848 - val_loss: 0.2801 - val_acc: 0.9369\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0204 - acc: 0.9924 - val_loss: 0.2706 - val_acc: 0.9295\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0156 - acc: 0.9957 - val_loss: 0.2078 - val_acc: 0.9692\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0266 - acc: 0.9917 - val_loss: 0.4078 - val_acc: 0.9249\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0318 - acc: 0.9926 - val_loss: 0.3843 - val_acc: 0.8898\n",
      "(Average) Test Accuracy= 0.540913648528\n",
      "(Average) Test Loss= 3.01700108837\n",
      "\n",
      "(round: 4 )\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0449 - acc: 0.9875 - val_loss: 0.4387 - val_acc: 0.9005\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0415 - acc: 0.9873 - val_loss: 0.3028 - val_acc: 0.9289\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0284 - acc: 0.9906 - val_loss: 0.2036 - val_acc: 0.9692\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0144 - acc: 0.9937 - val_loss: 0.3138 - val_acc: 0.9419\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0161 - acc: 0.9970 - val_loss: 0.4081 - val_acc: 0.9279\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0459 - acc: 0.9825 - val_loss: 0.6449 - val_acc: 0.8996\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1041/1041 [==============================] - 1s - loss: 0.0320 - acc: 0.9914 - val_loss: 0.4712 - val_acc: 0.8966\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0119 - acc: 0.9966 - val_loss: 0.2719 - val_acc: 0.9362\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0393 - acc: 0.9830 - val_loss: 0.3813 - val_acc: 0.8983\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0251 - acc: 0.9915 - val_loss: 0.5160 - val_acc: 0.9015\n",
      "(Average) Test Accuracy= 0.521257212598\n",
      "(Average) Test Loss= 2.9890761837\n",
      "\n",
      "(round: 5 )\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0218 - acc: 0.9924 - val_loss: 0.2564 - val_acc: 0.9362\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0076 - acc: 1.0000 - val_loss: 0.2188 - val_acc: 0.9658\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0270 - acc: 0.9913 - val_loss: 0.5278 - val_acc: 0.8955\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0157 - acc: 0.9956 - val_loss: 0.3405 - val_acc: 0.9318\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0231 - acc: 0.9926 - val_loss: 0.3422 - val_acc: 0.9068\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0092 - acc: 0.9962 - val_loss: 0.4731 - val_acc: 0.9249\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0145 - acc: 0.9962 - val_loss: 0.5199 - val_acc: 0.9015\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0330 - acc: 0.9858 - val_loss: 0.5908 - val_acc: 0.9039\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0646 - acc: 0.9817 - val_loss: 0.3295 - val_acc: 0.9207\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0784 - acc: 0.9789 - val_loss: 0.4683 - val_acc: 0.9042\n",
      "(Average) Test Accuracy= 0.528339845215\n",
      "(Average) Test Loss= 3.1073290216\n",
      "\n",
      "(round: 6 )\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0175 - acc: 0.9947 - val_loss: 0.3778 - val_acc: 0.9219\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0397 - acc: 0.9850 - val_loss: 0.4381 - val_acc: 0.9005\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0238 - acc: 0.9934 - val_loss: 0.5699 - val_acc: 0.8939\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0185 - acc: 0.9947 - val_loss: 0.3971 - val_acc: 0.8941\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0157 - acc: 0.9949 - val_loss: 0.1999 - val_acc: 0.9692\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0413 - acc: 0.9847 - val_loss: 0.5608 - val_acc: 0.9039\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0358 - acc: 0.9893 - val_loss: 0.3591 - val_acc: 0.9228\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0420 - acc: 0.9874 - val_loss: 0.2776 - val_acc: 0.9329\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0211 - acc: 0.9943 - val_loss: 0.2996 - val_acc: 0.9369\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0253 - acc: 0.9914 - val_loss: 0.4255 - val_acc: 0.9042\n",
      "(Average) Test Accuracy= 0.52771781404\n",
      "(Average) Test Loss= 3.02549786802\n",
      "\n",
      "(round: 7 )\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0189 - acc: 0.9933 - val_loss: 0.3304 - val_acc: 0.9161\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0195 - acc: 0.9924 - val_loss: 0.6131 - val_acc: 0.8939\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0313 - acc: 0.9904 - val_loss: 0.4863 - val_acc: 0.8889\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0131 - acc: 0.9970 - val_loss: 0.4470 - val_acc: 0.9219\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0438 - acc: 0.9825 - val_loss: 0.5542 - val_acc: 0.9005\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0266 - acc: 0.9949 - val_loss: 0.2036 - val_acc: 0.9692\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0274 - acc: 0.9908 - val_loss: 0.3352 - val_acc: 0.9167\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0450 - acc: 0.9867 - val_loss: 0.2955 - val_acc: 0.9394\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0278 - acc: 0.9923 - val_loss: 0.5598 - val_acc: 0.9127\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0352 - acc: 0.9915 - val_loss: 0.3215 - val_acc: 0.8941\n",
      "(Average) Test Accuracy= 0.519836414748\n",
      "(Average) Test Loss= 3.07658872826\n",
      "\n",
      "(round: 8 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0132 - acc: 0.9962 - val_loss: 0.2763 - val_acc: 0.9419\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0154 - acc: 0.9966 - val_loss: 0.2197 - val_acc: 0.9658\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0213 - acc: 0.9942 - val_loss: 0.4583 - val_acc: 0.9004\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0468 - acc: 0.9863 - val_loss: 0.5191 - val_acc: 0.8955\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0205 - acc: 0.9926 - val_loss: 0.3627 - val_acc: 0.8941\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0404 - acc: 0.9878 - val_loss: 0.3224 - val_acc: 0.9167\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0738 - acc: 0.9820 - val_loss: 0.4951 - val_acc: 0.9053\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0279 - acc: 0.9916 - val_loss: 0.2953 - val_acc: 0.9195\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0339 - acc: 0.9923 - val_loss: 0.5773 - val_acc: 0.9127\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0192 - acc: 0.9940 - val_loss: 0.3904 - val_acc: 0.9369\n",
      "(Average) Test Accuracy= 0.52774961435\n",
      "(Average) Test Loss= 3.07168855629\n",
      "\n",
      "(round: 9 )\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0129 - acc: 0.9955 - val_loss: 0.3922 - val_acc: 0.9399\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0152 - acc: 0.9953 - val_loss: 0.5191 - val_acc: 0.9129\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0158 - acc: 0.9952 - val_loss: 0.4408 - val_acc: 0.9004\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0093 - acc: 0.9983 - val_loss: 0.1962 - val_acc: 0.9692\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0252 - acc: 0.9930 - val_loss: 0.2894 - val_acc: 0.9419\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0215 - acc: 0.9908 - val_loss: 0.2607 - val_acc: 0.9362\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0195 - acc: 0.9915 - val_loss: 0.3687 - val_acc: 0.9025\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0184 - acc: 0.9963 - val_loss: 0.4870 - val_acc: 0.9104\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0282 - acc: 0.9934 - val_loss: 0.2943 - val_acc: 0.9187\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0491 - acc: 0.9847 - val_loss: 0.6440 - val_acc: 0.9083\n",
      "(Average) Test Accuracy= 0.534808285475\n",
      "(Average) Test Loss= 3.11039569107\n",
      "\n",
      "(round: 10 )\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0243 - acc: 0.9934 - val_loss: 0.6108 - val_acc: 0.9127\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0251 - acc: 0.9888 - val_loss: 0.5072 - val_acc: 0.9055\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0174 - acc: 0.9943 - val_loss: 0.3006 - val_acc: 0.9444\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0088 - acc: 0.9983 - val_loss: 0.1954 - val_acc: 0.9692\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0193 - acc: 0.9932 - val_loss: 0.3902 - val_acc: 0.9309\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0107 - acc: 0.9972 - val_loss: 0.5501 - val_acc: 0.9091\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0273 - acc: 0.9908 - val_loss: 0.3356 - val_acc: 0.9228\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0456 - acc: 0.9862 - val_loss: 0.2803 - val_acc: 0.9153\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0165 - acc: 0.9950 - val_loss: 0.2797 - val_acc: 0.9396\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0133 - acc: 0.9952 - val_loss: 0.5203 - val_acc: 0.8966\n",
      "(Average) Test Accuracy= 0.52113715046\n",
      "(Average) Test Loss= 3.18785292077\n",
      "\n",
      "(round: 11 )\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0074 - acc: 0.9991 - val_loss: 0.2060 - val_acc: 0.9692\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0241 - acc: 0.9908 - val_loss: 0.3457 - val_acc: 0.9146\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0609 - acc: 0.9846 - val_loss: 0.5118 - val_acc: 0.8966\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0246 - acc: 0.9908 - val_loss: 0.2599 - val_acc: 0.9396\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0250 - acc: 0.9896 - val_loss: 0.5241 - val_acc: 0.9053\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0236 - acc: 0.9918 - val_loss: 0.2984 - val_acc: 0.9444\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0252 - acc: 0.9904 - val_loss: 0.3105 - val_acc: 0.9068\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0320 - acc: 0.9913 - val_loss: 0.6397 - val_acc: 0.9039\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0310 - acc: 0.9875 - val_loss: 0.5639 - val_acc: 0.8905\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0343 - acc: 0.9917 - val_loss: 0.3809 - val_acc: 0.9369\n",
      "(Average) Test Accuracy= 0.532493969267\n",
      "(Average) Test Loss= 3.09566688089\n",
      "\n",
      "(round: 12 )\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0155 - acc: 0.9934 - val_loss: 0.5390 - val_acc: 0.9015\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0150 - acc: 0.9956 - val_loss: 0.3209 - val_acc: 0.9318\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0245 - acc: 0.9923 - val_loss: 0.4909 - val_acc: 0.9004\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0168 - acc: 0.9958 - val_loss: 0.2945 - val_acc: 0.9025\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0285 - acc: 0.9908 - val_loss: 0.3216 - val_acc: 0.9228\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0615 - acc: 0.9800 - val_loss: 0.4948 - val_acc: 0.9005\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0070 - acc: 0.9985 - val_loss: 0.4141 - val_acc: 0.9309\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0170 - acc: 0.9950 - val_loss: 0.3226 - val_acc: 0.9295\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0168 - acc: 0.9949 - val_loss: 0.1991 - val_acc: 0.9658\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0165 - acc: 0.9945 - val_loss: 0.7032 - val_acc: 0.9039\n",
      "(Average) Test Accuracy= 0.527404328992\n",
      "(Average) Test Loss= 3.06066573096\n",
      "\n",
      "(round: 13 )\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0183 - acc: 0.9943 - val_loss: 0.5036 - val_acc: 0.9053\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0212 - acc: 0.9934 - val_loss: 0.6915 - val_acc: 0.9039\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0220 - acc: 0.9930 - val_loss: 0.3344 - val_acc: 0.9268\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0251 - acc: 0.9913 - val_loss: 0.5035 - val_acc: 0.9005\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0159 - acc: 0.9955 - val_loss: 0.3962 - val_acc: 0.9339\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0232 - acc: 0.9936 - val_loss: 0.2951 - val_acc: 0.9110\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0189 - acc: 0.9942 - val_loss: 0.4659 - val_acc: 0.9042\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 1s - loss: 0.0098 - acc: 0.9966 - val_loss: 0.1949 - val_acc: 0.9692\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0126 - acc: 0.9950 - val_loss: 0.3167 - val_acc: 0.9295\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0226 - acc: 0.9924 - val_loss: 0.3272 - val_acc: 0.9289\n",
      "(Average) Test Accuracy= 0.52415569352\n",
      "(Average) Test Loss= 3.19140561424\n",
      "\n",
      "(round: 14 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0227 - acc: 0.9943 - val_loss: 0.3293 - val_acc: 0.9470\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0119 - acc: 0.9966 - val_loss: 0.1950 - val_acc: 0.9658\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0344 - acc: 0.9873 - val_loss: 0.3319 - val_acc: 0.9207\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0318 - acc: 0.9916 - val_loss: 0.2888 - val_acc: 0.9295\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0132 - acc: 0.9940 - val_loss: 0.4100 - val_acc: 0.9339\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0309 - acc: 0.9925 - val_loss: 0.5184 - val_acc: 0.9104\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0277 - acc: 0.9913 - val_loss: 0.6030 - val_acc: 0.9170\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0275 - acc: 0.9896 - val_loss: 0.6739 - val_acc: 0.8902\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0294 - acc: 0.9926 - val_loss: 0.2934 - val_acc: 0.9110\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0137 - acc: 0.9962 - val_loss: 0.5193 - val_acc: 0.9004\n",
      "(Average) Test Accuracy= 0.533350505939\n",
      "(Average) Test Loss= 3.2173922802\n",
      "\n",
      "(round: 15 )\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0326 - acc: 0.9938 - val_loss: 0.4892 - val_acc: 0.9104\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0118 - acc: 0.9966 - val_loss: 0.2062 - val_acc: 0.9658\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0145 - acc: 0.9953 - val_loss: 0.6118 - val_acc: 0.8977\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0120 - acc: 0.9975 - val_loss: 0.3266 - val_acc: 0.9343\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0113 - acc: 0.9971 - val_loss: 0.5403 - val_acc: 0.8851\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0194 - acc: 0.9950 - val_loss: 0.3257 - val_acc: 0.9295\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0238 - acc: 0.9947 - val_loss: 0.3293 - val_acc: 0.9068\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0111 - acc: 0.9977 - val_loss: 0.4345 - val_acc: 0.9339\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0124 - acc: 0.9967 - val_loss: 0.6266 - val_acc: 0.9170\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0255 - acc: 0.9914 - val_loss: 0.3435 - val_acc: 0.9248\n",
      "(Average) Test Accuracy= 0.529570226867\n",
      "(Average) Test Loss= 3.14892183498\n",
      "\n",
      "(round: 16 )\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0181 - acc: 0.9939 - val_loss: 0.3258 - val_acc: 0.9248\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0272 - acc: 0.9902 - val_loss: 0.4406 - val_acc: 0.9309\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0320 - acc: 0.9900 - val_loss: 0.4859 - val_acc: 0.8806\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0169 - acc: 0.9945 - val_loss: 0.6218 - val_acc: 0.8996\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0206 - acc: 0.9941 - val_loss: 0.2669 - val_acc: 0.9463\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0207 - acc: 0.9949 - val_loss: 0.3394 - val_acc: 0.9394\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0112 - acc: 0.9966 - val_loss: 0.2095 - val_acc: 0.9658\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0150 - acc: 0.9943 - val_loss: 0.5570 - val_acc: 0.8977\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0221 - acc: 0.9936 - val_loss: 0.3179 - val_acc: 0.8941\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0111 - acc: 0.9962 - val_loss: 0.4879 - val_acc: 0.8966\n",
      "(Average) Test Accuracy= 0.532624180942\n",
      "(Average) Test Loss= 3.19390508817\n",
      "\n",
      "(round: 17 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0173 - acc: 0.9943 - val_loss: 0.3693 - val_acc: 0.9369\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0064 - acc: 0.9989 - val_loss: 0.6757 - val_acc: 0.9083\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0065 - acc: 0.9991 - val_loss: 0.2204 - val_acc: 0.9692\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0260 - acc: 0.9933 - val_loss: 0.4692 - val_acc: 0.9042\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0350 - acc: 0.9850 - val_loss: 0.5264 - val_acc: 0.9154\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0050 - acc: 0.9991 - val_loss: 0.5649 - val_acc: 0.9053\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0151 - acc: 0.9958 - val_loss: 0.3150 - val_acc: 0.9329\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0102 - acc: 0.9940 - val_loss: 0.4077 - val_acc: 0.9339\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0143 - acc: 0.9968 - val_loss: 0.3310 - val_acc: 0.9153\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0266 - acc: 0.9929 - val_loss: 0.3594 - val_acc: 0.9289\n",
      "(Average) Test Accuracy= 0.533588777414\n",
      "(Average) Test Loss= 3.25669119689\n",
      "\n",
      "(round: 18 )\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0273 - acc: 0.9895 - val_loss: 0.4653 - val_acc: 0.9369\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0247 - acc: 0.9900 - val_loss: 0.4176 - val_acc: 0.9055\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0124 - acc: 0.9967 - val_loss: 0.6491 - val_acc: 0.9127\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0125 - acc: 0.9956 - val_loss: 0.3344 - val_acc: 0.9470\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0061 - acc: 0.9983 - val_loss: 0.2748 - val_acc: 0.9430\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0123 - acc: 0.9972 - val_loss: 0.5953 - val_acc: 0.8939\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0072 - acc: 0.9989 - val_loss: 0.3301 - val_acc: 0.9025\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0141 - acc: 0.9959 - val_loss: 0.3284 - val_acc: 0.9228\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0567 - acc: 0.9856 - val_loss: 0.4836 - val_acc: 0.8927\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0127 - acc: 0.9966 - val_loss: 0.2342 - val_acc: 0.9658\n",
      "(Average) Test Accuracy= 0.538928696404\n",
      "(Average) Test Loss= 3.26275722052\n",
      "\n",
      "(round: 19 )\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0171 - acc: 0.9924 - val_loss: 0.2779 - val_acc: 0.9329\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0092 - acc: 0.9971 - val_loss: 0.4997 - val_acc: 0.9042\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0340 - acc: 0.9938 - val_loss: 0.4034 - val_acc: 0.9303\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0072 - acc: 0.9983 - val_loss: 0.2313 - val_acc: 0.9658\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0152 - acc: 0.9958 - val_loss: 0.5516 - val_acc: 0.8941\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0207 - acc: 0.9914 - val_loss: 0.3522 - val_acc: 0.9268\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0285 - acc: 0.9924 - val_loss: 0.3032 - val_acc: 0.9293\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0144 - acc: 0.9956 - val_loss: 0.6409 - val_acc: 0.9127\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0154 - acc: 0.9934 - val_loss: 0.6022 - val_acc: 0.9053\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0229 - acc: 0.9917 - val_loss: 0.3898 - val_acc: 0.9369\n",
      "(Average) Test Accuracy= 0.536723959129\n",
      "(Average) Test Loss= 3.27790385492\n",
      "\n",
      "It cost 666.568990 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tStart = time.time()#計時開始\n",
    "\n",
    "#for i in range(0,10,1):\n",
    "epoch_num=20\n",
    "for j in range(0,epoch_num,1):\n",
    "    testarr=numpy.arange(0, 10, 1)\n",
    "    random.shuffle(testarr)\n",
    "    \n",
    "    arr_accuracy=np.zeros(10)\n",
    "    arr_loss=np.zeros(10)\n",
    "    arr_num=np.zeros(10)\n",
    "    print(\"(round:\",j,\")\")\n",
    "    for ii in range(0,10,1):\n",
    "        i=testarr[ii]\n",
    "        \n",
    "        Lmystr=\"NewLtest\"+str(i)+\".npy\"\n",
    "        Rmystr=\"NewRtest\"+str(i)+\".npy\"\n",
    "        LX_test=np.load(Lmystr)\n",
    "        RX_test=np.load(Rmystr)\n",
    "        x_img_test=np.concatenate((LX_test, RX_test), axis=0)\n",
    "\n",
    "        Lmystr2=\"NewLtrain\"+str(i)+\".npy\"\n",
    "        Rmystr2=\"NewRtrain\"+str(i)+\".npy\"\n",
    "        LX_train=np.load(Lmystr2)\n",
    "        RX_train=np.load(Rmystr2)\n",
    "        x_img_train=np.concatenate((LX_train, RX_train), axis=0)\n",
    "\n",
    "        Ltempx=np.load(Lobj_label_DATA_DIR[i])    \n",
    "        Rtempx=np.load(Robj_label_DATA_DIR[i])\n",
    "        y_label_train=np.concatenate((Ltempx, Rtempx), axis=0)\n",
    "\n",
    "        TLtempx=np.load(TLobj_label_DATA_DIR[i])    \n",
    "        TRtempx=np.load(TRobj_label_DATA_DIR[i])\n",
    "        y_label_test=np.concatenate((TLtempx, TRtempx), axis=0)\n",
    "\n",
    "        #Shuffle\n",
    "        x_img_train, y_label_train = shuffle(x_img_train, y_label_train, random_state=0)\n",
    "        x_img_test, y_label_test = shuffle(x_img_test, y_label_test, random_state=0)\n",
    "\n",
    "        #Image normalize\n",
    "        x_img_train_normalize = x_img_train.astype('float32') / 255.0\n",
    "        x_img_test_normalize = x_img_test.astype('float32') / 255.0\n",
    "\n",
    "        #轉換label 為OneHot Encoding\n",
    "        y_label_train_OneHot = np_utils.to_categorical(y_label_train, num_classes=24)\n",
    "        y_label_test_OneHot = np_utils.to_categorical(y_label_test, num_classes=24)\n",
    "\n",
    "        #print: train data/test data\n",
    "        #print(\"(round:\",j,\", dataset:\",i,\")\")\n",
    "        #print(\"train data:\",'images:',x_img_train.shape,\n",
    "        #      \" labels:\",y_label_train.shape) \n",
    "        #print(\"test  data:\",'images:',x_img_test.shape ,\n",
    "        #      \" labels:\",y_label_test.shape)\n",
    "\n",
    "        train_history=model.fit(x_img_train_normalize, y_label_train_OneHot, \n",
    "                                validation_split=0.2, epochs=1, \n",
    "                                batch_size=64, verbose=1)\n",
    "        \n",
    "        save_train_history(train_history,'acc','val_acc')\n",
    "        save_train_history2(train_history,'loss','val_loss')\n",
    "               \n",
    "        score = model.evaluate(x_img_test_normalize, y_label_test_OneHot, verbose=0)\n",
    "        #arr_num[i]=x_img_train_normalize.shape[0]       \n",
    "        \n",
    "        arr_num[i]=x_img_train_normalize.shape[0]       \n",
    "        arr_accuracy[i]=score[1]\n",
    "        arr_loss[i]=score[0]\n",
    "    \n",
    "    average_accuracy=np.average(arr_accuracy, weights=arr_num)\n",
    "    print(\"(Average) Test Accuracy=\",average_accuracy)\n",
    "\n",
    "    average_loss=np.average(arr_loss, weights=arr_num)\n",
    "    print(\"(Average) Test Loss=\",average_loss)\n",
    "    print(\"\")\n",
    "    testaccList.append(average_accuracy)\n",
    "    teslossList.append(average_loss)\n",
    "        \n",
    "        #score = model.evaluate(x_img_train_normalize, y_label_train_OneHot, verbose=0)\n",
    "        #print('Test loss:', score[0])\n",
    "        #print('Test accuracy:', score[1])\n",
    "        \n",
    "        #arr_num[i]=x_img_train_normalize.shape[0]       \n",
    "        #arr_accuracy[i]=score[1]\n",
    "        #arr_loss[i]=score[0]\n",
    "tEnd = time.time() #計時結束\n",
    "print (\"It cost %f sec\" % (tEnd - tStart))#會自動做近位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(round: 0 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0102 - acc: 0.9981 - val_loss: 0.3574 - val_acc: 0.9419\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0105 - acc: 0.9972 - val_loss: 0.4983 - val_acc: 0.9091\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0090 - acc: 0.9974 - val_loss: 0.2141 - val_acc: 0.9658\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0183 - acc: 0.9947 - val_loss: 0.4257 - val_acc: 0.9339\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0115 - acc: 0.9963 - val_loss: 0.4702 - val_acc: 0.9104\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0245 - acc: 0.9914 - val_loss: 0.3482 - val_acc: 0.9268\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0342 - acc: 0.9894 - val_loss: 0.4362 - val_acc: 0.9119\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0227 - acc: 0.9883 - val_loss: 0.4182 - val_acc: 0.9110\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0107 - acc: 0.9966 - val_loss: 0.3042 - val_acc: 0.9329\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0171 - acc: 0.9967 - val_loss: 0.6403 - val_acc: 0.9083\n",
      "(Average) Test Accuracy= 0.530709440754\n",
      "(Average) Test Loss= 3.31984825649\n",
      "\n",
      "(round: 1 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0116 - acc: 0.9956 - val_loss: 0.2992 - val_acc: 0.9444\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0218 - acc: 0.9934 - val_loss: 0.5663 - val_acc: 0.9053\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0189 - acc: 0.9942 - val_loss: 0.4857 - val_acc: 0.9042\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0204 - acc: 0.9932 - val_loss: 0.4069 - val_acc: 0.9339\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0106 - acc: 0.9968 - val_loss: 0.3259 - val_acc: 0.9110\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0083 - acc: 0.9975 - val_loss: 0.2786 - val_acc: 0.9463\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0158 - acc: 0.9950 - val_loss: 0.5026 - val_acc: 0.9055\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0295 - acc: 0.9956 - val_loss: 0.6318 - val_acc: 0.9083\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0112 - acc: 0.9949 - val_loss: 0.2388 - val_acc: 0.9692\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0199 - acc: 0.9934 - val_loss: 0.3212 - val_acc: 0.9268\n",
      "(Average) Test Accuracy= 0.520629244855\n",
      "(Average) Test Loss= 3.37932928699\n",
      "\n",
      "(round: 2 )\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0493 - acc: 0.9880 - val_loss: 0.6726 - val_acc: 0.8952\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0232 - acc: 0.9911 - val_loss: 0.3260 - val_acc: 0.9369\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0502 - acc: 0.9813 - val_loss: 0.4813 - val_acc: 0.9104\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0105 - acc: 0.9962 - val_loss: 0.4783 - val_acc: 0.9004\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0209 - acc: 0.9939 - val_loss: 0.3487 - val_acc: 0.9167\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0274 - acc: 0.9941 - val_loss: 0.2766 - val_acc: 0.9295\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0166 - acc: 0.9958 - val_loss: 0.2879 - val_acc: 0.9237\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0130 - acc: 0.9981 - val_loss: 0.5535 - val_acc: 0.9091\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0105 - acc: 0.9947 - val_loss: 0.3876 - val_acc: 0.9279\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0052 - acc: 0.9991 - val_loss: 0.2260 - val_acc: 0.9658\n",
      "(Average) Test Accuracy= 0.532712670205\n",
      "(Average) Test Loss= 3.23918453003\n",
      "\n",
      "(round: 3 )\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0132 - acc: 0.9952 - val_loss: 0.5217 - val_acc: 0.9004\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0185 - acc: 0.9956 - val_loss: 0.3343 - val_acc: 0.9318\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0064 - acc: 0.9989 - val_loss: 0.3209 - val_acc: 0.9110\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0059 - acc: 1.0000 - val_loss: 0.2844 - val_acc: 0.9329\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0040 - acc: 1.0000 - val_loss: 0.4163 - val_acc: 0.9309\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0231 - acc: 0.9945 - val_loss: 0.6543 - val_acc: 0.9214\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0199 - acc: 0.9934 - val_loss: 0.3531 - val_acc: 0.9329\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0110 - acc: 0.9966 - val_loss: 0.2346 - val_acc: 0.9623\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0188 - acc: 0.9943 - val_loss: 0.6784 - val_acc: 0.8977\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0290 - acc: 0.9888 - val_loss: 0.5113 - val_acc: 0.9055\n",
      "(Average) Test Accuracy= 0.524254555057\n",
      "(Average) Test Loss= 3.36881079064\n",
      "\n",
      "(round: 4 )\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0108 - acc: 0.9966 - val_loss: 0.4395 - val_acc: 0.9027\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0452 - acc: 0.9875 - val_loss: 0.4899 - val_acc: 0.9104\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0366 - acc: 0.9913 - val_loss: 0.6439 - val_acc: 0.8996\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0064 - acc: 0.9981 - val_loss: 0.6338 - val_acc: 0.9015\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0087 - acc: 0.9974 - val_loss: 0.2289 - val_acc: 0.9623\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0152 - acc: 0.9962 - val_loss: 0.3396 - val_acc: 0.9444\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1966/1966 [==============================] - 2s - loss: 0.0259 - acc: 0.9908 - val_loss: 0.3926 - val_acc: 0.9187\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0181 - acc: 0.9940 - val_loss: 0.4195 - val_acc: 0.9399\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0132 - acc: 0.9947 - val_loss: 0.3477 - val_acc: 0.9110\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0144 - acc: 0.9933 - val_loss: 0.5460 - val_acc: 0.8889\n",
      "(Average) Test Accuracy= 0.530576835038\n",
      "(Average) Test Loss= 3.27786717256\n",
      "\n",
      "(round: 5 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0097 - acc: 0.9968 - val_loss: 0.3495 - val_acc: 0.9470\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0125 - acc: 0.9966 - val_loss: 0.2844 - val_acc: 0.9295\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0072 - acc: 0.9985 - val_loss: 0.4321 - val_acc: 0.9339\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0049 - acc: 0.9990 - val_loss: 0.5014 - val_acc: 0.8966\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0150 - acc: 0.9939 - val_loss: 0.3787 - val_acc: 0.9268\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0279 - acc: 0.9932 - val_loss: 0.2503 - val_acc: 0.9623\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0112 - acc: 0.9953 - val_loss: 0.6155 - val_acc: 0.9015\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0167 - acc: 0.9989 - val_loss: 0.6676 - val_acc: 0.8865\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0185 - acc: 0.9975 - val_loss: 0.5243 - val_acc: 0.9055\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0135 - acc: 0.9947 - val_loss: 0.3249 - val_acc: 0.9237\n",
      "(Average) Test Accuracy= 0.533611151084\n",
      "(Average) Test Loss= 3.3323108863\n",
      "\n",
      "(round: 6 )\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0109 - acc: 0.9955 - val_loss: 0.4404 - val_acc: 0.9369\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0089 - acc: 0.9983 - val_loss: 0.3004 - val_acc: 0.9329\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0383 - acc: 0.9900 - val_loss: 0.6336 - val_acc: 0.9104\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0161 - acc: 0.9915 - val_loss: 0.3621 - val_acc: 0.9153\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0254 - acc: 0.9944 - val_loss: 0.3719 - val_acc: 0.9228\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0286 - acc: 0.9899 - val_loss: 0.3663 - val_acc: 0.9293\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0259 - acc: 0.9891 - val_loss: 0.9128 - val_acc: 0.8734\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0123 - acc: 0.9957 - val_loss: 0.2477 - val_acc: 0.9623\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0048 - acc: 0.9991 - val_loss: 0.6240 - val_acc: 0.9015\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0177 - acc: 0.9942 - val_loss: 0.5464 - val_acc: 0.8851\n",
      "(Average) Test Accuracy= 0.529530213618\n",
      "(Average) Test Loss= 3.28810924211\n",
      "\n",
      "(round: 7 )\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0076 - acc: 0.9970 - val_loss: 0.4618 - val_acc: 0.9429\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0228 - acc: 0.9908 - val_loss: 0.3672 - val_acc: 0.9268\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0233 - acc: 0.9925 - val_loss: 0.5089 - val_acc: 0.9104\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0147 - acc: 0.9956 - val_loss: 0.6345 - val_acc: 0.8996\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0060 - acc: 0.9975 - val_loss: 0.2858 - val_acc: 0.9362\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0092 - acc: 0.9966 - val_loss: 0.2369 - val_acc: 0.9658\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0191 - acc: 0.9947 - val_loss: 0.3651 - val_acc: 0.9025\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0093 - acc: 0.9956 - val_loss: 0.3685 - val_acc: 0.9444\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0069 - acc: 0.9990 - val_loss: 0.5136 - val_acc: 0.8927\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0088 - acc: 0.9972 - val_loss: 0.6318 - val_acc: 0.8977\n",
      "(Average) Test Accuracy= 0.535965988272\n",
      "(Average) Test Loss= 3.32344202661\n",
      "\n",
      "(round: 8 )\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0109 - acc: 0.9975 - val_loss: 0.4059 - val_acc: 0.9187\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0245 - acc: 0.9936 - val_loss: 0.3637 - val_acc: 0.8941\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0543 - acc: 0.9863 - val_loss: 0.5776 - val_acc: 0.9055\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0159 - acc: 0.9962 - val_loss: 0.6067 - val_acc: 0.9091\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0054 - acc: 0.9974 - val_loss: 0.2101 - val_acc: 0.9623\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0212 - acc: 0.9956 - val_loss: 0.6602 - val_acc: 0.9039\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0080 - acc: 0.9966 - val_loss: 0.3218 - val_acc: 0.9329\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0195 - acc: 0.9949 - val_loss: 0.3183 - val_acc: 0.9470\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0178 - acc: 0.9947 - val_loss: 0.4553 - val_acc: 0.9399\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0125 - acc: 0.9962 - val_loss: 0.4982 - val_acc: 0.9004\n",
      "(Average) Test Accuracy= 0.53282169492\n",
      "(Average) Test Loss= 3.30654179921\n",
      "\n",
      "(round: 9 )\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0127 - acc: 0.9967 - val_loss: 0.6934 - val_acc: 0.8865\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0039 - acc: 0.9991 - val_loss: 0.2236 - val_acc: 0.9589\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0229 - acc: 0.9913 - val_loss: 0.5133 - val_acc: 0.9104\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0091 - acc: 0.9981 - val_loss: 0.3018 - val_acc: 0.9444\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0068 - acc: 0.9971 - val_loss: 0.4786 - val_acc: 0.9004\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0216 - acc: 0.9908 - val_loss: 0.3781 - val_acc: 0.9207\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0156 - acc: 0.9943 - val_loss: 0.6739 - val_acc: 0.9015\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0136 - acc: 0.9966 - val_loss: 0.2825 - val_acc: 0.9295\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0104 - acc: 0.9962 - val_loss: 0.4334 - val_acc: 0.9309\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0279 - acc: 0.9936 - val_loss: 0.3406 - val_acc: 0.8983\n",
      "(Average) Test Accuracy= 0.539456162445\n",
      "(Average) Test Loss= 3.27269280106\n",
      "\n",
      "(round: 10 )\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0036 - acc: 1.0000 - val_loss: 0.3309 - val_acc: 0.9025\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0044 - acc: 0.9983 - val_loss: 0.2104 - val_acc: 0.9658\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0130 - acc: 0.9962 - val_loss: 0.3171 - val_acc: 0.9444\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0184 - acc: 0.9945 - val_loss: 0.6494 - val_acc: 0.9039\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0179 - acc: 0.9959 - val_loss: 0.3646 - val_acc: 0.9146\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0155 - acc: 0.9958 - val_loss: 0.2992 - val_acc: 0.9329\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0213 - acc: 0.9938 - val_loss: 0.5272 - val_acc: 0.8905\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0108 - acc: 0.9971 - val_loss: 0.4677 - val_acc: 0.8889\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0070 - acc: 0.9977 - val_loss: 0.4270 - val_acc: 0.9369\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0083 - acc: 0.9972 - val_loss: 0.6288 - val_acc: 0.8902\n",
      "(Average) Test Accuracy= 0.537965979551\n",
      "(Average) Test Loss= 3.28410341493\n",
      "\n",
      "(round: 11 )\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0127 - acc: 0.9962 - val_loss: 0.4960 - val_acc: 0.9004\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0085 - acc: 0.9979 - val_loss: 0.3687 - val_acc: 0.9110\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0155 - acc: 0.9954 - val_loss: 0.3741 - val_acc: 0.9268\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0166 - acc: 0.9950 - val_loss: 0.2852 - val_acc: 0.9362\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0049 - acc: 0.9985 - val_loss: 0.4187 - val_acc: 0.9459\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0218 - acc: 0.9888 - val_loss: 0.6049 - val_acc: 0.9055\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0087 - acc: 0.9978 - val_loss: 0.7231 - val_acc: 0.8734\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0089 - acc: 0.9974 - val_loss: 0.2160 - val_acc: 0.9692\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0077 - acc: 0.9991 - val_loss: 0.6600 - val_acc: 0.9015\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0063 - acc: 0.9981 - val_loss: 0.3097 - val_acc: 0.9394\n",
      "(Average) Test Accuracy= 0.533205869461\n",
      "(Average) Test Loss= 3.33584065494\n",
      "\n",
      "(round: 12 )\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0080 - acc: 0.9975 - val_loss: 0.3048 - val_acc: 0.9362\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0057 - acc: 0.9981 - val_loss: 0.4774 - val_acc: 0.9004\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0095 - acc: 0.9970 - val_loss: 0.4663 - val_acc: 0.9369\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0160 - acc: 0.9954 - val_loss: 0.3669 - val_acc: 0.9126\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0393 - acc: 0.9891 - val_loss: 0.6205 - val_acc: 0.9127\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0200 - acc: 0.9943 - val_loss: 0.6199 - val_acc: 0.9015\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0047 - acc: 0.9991 - val_loss: 0.2093 - val_acc: 0.9658\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0072 - acc: 0.9968 - val_loss: 0.3420 - val_acc: 0.9369\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0354 - acc: 0.9925 - val_loss: 0.6056 - val_acc: 0.9005\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0076 - acc: 0.9979 - val_loss: 0.3681 - val_acc: 0.8983\n",
      "(Average) Test Accuracy= 0.538486646332\n",
      "(Average) Test Loss= 3.30988279953\n",
      "\n",
      "(round: 13 )\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0119 - acc: 0.9968 - val_loss: 0.3345 - val_acc: 0.9419\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0175 - acc: 0.9934 - val_loss: 0.3734 - val_acc: 0.9207\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0220 - acc: 0.9908 - val_loss: 0.2712 - val_acc: 0.9396\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0157 - acc: 0.9952 - val_loss: 0.6398 - val_acc: 0.8697\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0060 - acc: 0.9991 - val_loss: 0.2325 - val_acc: 0.9589\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0050 - acc: 0.9989 - val_loss: 0.3738 - val_acc: 0.8983\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0106 - acc: 0.9975 - val_loss: 0.5589 - val_acc: 0.9104\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1329/1329 [==============================] - 1s - loss: 0.0028 - acc: 1.0000 - val_loss: 0.4272 - val_acc: 0.9429\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0098 - acc: 0.9967 - val_loss: 0.7152 - val_acc: 0.8952\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0125 - acc: 0.9972 - val_loss: 0.6429 - val_acc: 0.9053\n",
      "(Average) Test Accuracy= 0.535077846467\n",
      "(Average) Test Loss= 3.34976682291\n",
      "\n",
      "(round: 14 )\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0063 - acc: 0.9981 - val_loss: 0.5442 - val_acc: 0.8927\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0090 - acc: 0.9963 - val_loss: 0.5569 - val_acc: 0.9055\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0121 - acc: 0.9964 - val_loss: 0.3971 - val_acc: 0.9187\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0185 - acc: 0.9962 - val_loss: 0.4239 - val_acc: 0.9429\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0114 - acc: 0.9966 - val_loss: 0.3058 - val_acc: 0.9362\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0120 - acc: 0.9956 - val_loss: 0.7187 - val_acc: 0.9039\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0209 - acc: 0.9949 - val_loss: 0.3750 - val_acc: 0.9394\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0065 - acc: 0.9991 - val_loss: 0.2475 - val_acc: 0.9623\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0029 - acc: 1.0000 - val_loss: 0.6530 - val_acc: 0.8977\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0156 - acc: 0.9936 - val_loss: 0.3229 - val_acc: 0.9110\n",
      "(Average) Test Accuracy= 0.541553241303\n",
      "(Average) Test Loss= 3.43139602775\n",
      "\n",
      "(round: 15 )\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0147 - acc: 0.9950 - val_loss: 0.5419 - val_acc: 0.9204\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0126 - acc: 0.9964 - val_loss: 0.3867 - val_acc: 0.9248\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0273 - acc: 0.9936 - val_loss: 0.3527 - val_acc: 0.9025\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0061 - acc: 1.0000 - val_loss: 0.3103 - val_acc: 0.9161\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0072 - acc: 0.9989 - val_loss: 0.7666 - val_acc: 0.8952\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0028 - acc: 1.0000 - val_loss: 0.5668 - val_acc: 0.9004\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0044 - acc: 0.9991 - val_loss: 0.2514 - val_acc: 0.9623\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0114 - acc: 0.9987 - val_loss: 0.3515 - val_acc: 0.9394\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0033 - acc: 0.9985 - val_loss: 0.4168 - val_acc: 0.9369\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0026 - acc: 1.0000 - val_loss: 0.6587 - val_acc: 0.9015\n",
      "(Average) Test Accuracy= 0.53199363487\n",
      "(Average) Test Loss= 3.48651635246\n",
      "\n",
      "(round: 16 )\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0065 - acc: 0.9983 - val_loss: 0.3181 - val_acc: 0.9295\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0054 - acc: 0.9991 - val_loss: 0.6483 - val_acc: 0.9053\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0080 - acc: 0.9962 - val_loss: 0.4406 - val_acc: 0.9369\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0036 - acc: 0.9974 - val_loss: 0.2493 - val_acc: 0.9692\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0043 - acc: 0.9989 - val_loss: 0.3371 - val_acc: 0.9068\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0090 - acc: 0.9989 - val_loss: 0.7210 - val_acc: 0.8952\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0048 - acc: 0.9981 - val_loss: 0.5344 - val_acc: 0.8927\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0134 - acc: 0.9934 - val_loss: 0.4096 - val_acc: 0.9106\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0158 - acc: 0.9968 - val_loss: 0.4152 - val_acc: 0.9419\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0098 - acc: 0.9963 - val_loss: 0.5795 - val_acc: 0.9104\n",
      "(Average) Test Accuracy= 0.541393674399\n",
      "(Average) Test Loss= 3.50473813263\n",
      "\n",
      "(round: 17 )\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0109 - acc: 0.9969 - val_loss: 0.4069 - val_acc: 0.9187\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0262 - acc: 0.9904 - val_loss: 0.5909 - val_acc: 0.9004\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0226 - acc: 0.9924 - val_loss: 0.6899 - val_acc: 0.8977\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0252 - acc: 0.9936 - val_loss: 0.3761 - val_acc: 0.9068\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0138 - acc: 0.9949 - val_loss: 0.3938 - val_acc: 0.9369\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0044 - acc: 0.9991 - val_loss: 0.2368 - val_acc: 0.9623\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0141 - acc: 0.9963 - val_loss: 0.5200 - val_acc: 0.9055\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0078 - acc: 0.9978 - val_loss: 0.6821 - val_acc: 0.9039\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0088 - acc: 0.9983 - val_loss: 0.2668 - val_acc: 0.9362\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0107 - acc: 0.9977 - val_loss: 0.4585 - val_acc: 0.9459\n",
      "(Average) Test Accuracy= 0.529039045141\n",
      "(Average) Test Loss= 3.51958051144\n",
      "\n",
      "(round: 18 )\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0081 - acc: 0.9978 - val_loss: 0.6769 - val_acc: 0.9039\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0107 - acc: 0.9957 - val_loss: 0.2360 - val_acc: 0.9726\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0153 - acc: 0.9934 - val_loss: 0.3758 - val_acc: 0.9207\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0114 - acc: 0.9968 - val_loss: 0.3539 - val_acc: 0.9153\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0073 - acc: 0.9981 - val_loss: 0.5520 - val_acc: 0.8966\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0153 - acc: 0.9949 - val_loss: 0.3685 - val_acc: 0.9419\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0048 - acc: 0.9992 - val_loss: 0.2813 - val_acc: 0.9329\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0123 - acc: 0.9962 - val_loss: 0.4397 - val_acc: 0.9399\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0149 - acc: 0.9924 - val_loss: 0.6271 - val_acc: 0.9053\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0073 - acc: 0.9988 - val_loss: 0.5468 - val_acc: 0.9154\n",
      "(Average) Test Accuracy= 0.531168733105\n",
      "(Average) Test Loss= 3.52482145933\n",
      "\n",
      "(round: 19 )\n",
      "Train on 1329 samples, validate on 333 samples\n",
      "Epoch 1/1\n",
      "1329/1329 [==============================] - 1s - loss: 0.0076 - acc: 0.9962 - val_loss: 0.4704 - val_acc: 0.9339\n",
      "Train on 1192 samples, validate on 298 samples\n",
      "Epoch 1/1\n",
      "1192/1192 [==============================] - 1s - loss: 0.0050 - acc: 0.9983 - val_loss: 0.2760 - val_acc: 0.9396\n",
      "Train on 915 samples, validate on 229 samples\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 1s - loss: 0.0119 - acc: 0.9956 - val_loss: 0.7197 - val_acc: 0.9083\n",
      "Train on 1580 samples, validate on 396 samples\n",
      "Epoch 1/1\n",
      "1580/1580 [==============================] - 1s - loss: 0.0121 - acc: 0.9968 - val_loss: 0.3973 - val_acc: 0.9369\n",
      "Train on 1056 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "1056/1056 [==============================] - 1s - loss: 0.0045 - acc: 0.9991 - val_loss: 0.6886 - val_acc: 0.9015\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1\n",
      "1168/1168 [==============================] - 1s - loss: 0.0055 - acc: 0.9983 - val_loss: 0.2198 - val_acc: 0.9658\n",
      "Train on 801 samples, validate on 201 samples\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s - loss: 0.0263 - acc: 0.9913 - val_loss: 0.5462 - val_acc: 0.9154\n",
      "Train on 1041 samples, validate on 261 samples\n",
      "Epoch 1/1\n",
      "1041/1041 [==============================] - 1s - loss: 0.0094 - acc: 0.9981 - val_loss: 0.5576 - val_acc: 0.8966\n",
      "Train on 942 samples, validate on 236 samples\n",
      "Epoch 1/1\n",
      "942/942 [==============================] - 1s - loss: 0.0039 - acc: 0.9989 - val_loss: 0.3578 - val_acc: 0.9025\n",
      "Train on 1966 samples, validate on 492 samples\n",
      "Epoch 1/1\n",
      "1966/1966 [==============================] - 2s - loss: 0.0121 - acc: 0.9964 - val_loss: 0.3912 - val_acc: 0.9248\n",
      "(Average) Test Accuracy= 0.522940131971\n",
      "(Average) Test Loss= 3.52182794571\n",
      "\n",
      "It cost 641.038933 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tStart = time.time()#計時開始\n",
    "\n",
    "#for i in range(0,10,1):\n",
    "epoch_num=20\n",
    "for j in range(0,epoch_num,1):\n",
    "    testarr=numpy.arange(0, 10, 1)\n",
    "    random.shuffle(testarr)\n",
    "    \n",
    "    arr_accuracy=np.zeros(10)\n",
    "    arr_loss=np.zeros(10)\n",
    "    arr_num=np.zeros(10)\n",
    "    print(\"(round:\",j,\")\")\n",
    "    for ii in range(0,10,1):\n",
    "        i=testarr[ii]\n",
    "        \n",
    "        Lmystr=\"NewLtest\"+str(i)+\".npy\"\n",
    "        Rmystr=\"NewRtest\"+str(i)+\".npy\"\n",
    "        LX_test=np.load(Lmystr)\n",
    "        RX_test=np.load(Rmystr)\n",
    "        x_img_test=np.concatenate((LX_test, RX_test), axis=0)\n",
    "\n",
    "        Lmystr2=\"NewLtrain\"+str(i)+\".npy\"\n",
    "        Rmystr2=\"NewRtrain\"+str(i)+\".npy\"\n",
    "        LX_train=np.load(Lmystr2)\n",
    "        RX_train=np.load(Rmystr2)\n",
    "        x_img_train=np.concatenate((LX_train, RX_train), axis=0)\n",
    "\n",
    "        Ltempx=np.load(Lobj_label_DATA_DIR[i])    \n",
    "        Rtempx=np.load(Robj_label_DATA_DIR[i])\n",
    "        y_label_train=np.concatenate((Ltempx, Rtempx), axis=0)\n",
    "\n",
    "        TLtempx=np.load(TLobj_label_DATA_DIR[i])    \n",
    "        TRtempx=np.load(TRobj_label_DATA_DIR[i])\n",
    "        y_label_test=np.concatenate((TLtempx, TRtempx), axis=0)\n",
    "\n",
    "        #Shuffle\n",
    "        x_img_train, y_label_train = shuffle(x_img_train, y_label_train, random_state=0)\n",
    "        x_img_test, y_label_test = shuffle(x_img_test, y_label_test, random_state=0)\n",
    "\n",
    "        #Image normalize\n",
    "        x_img_train_normalize = x_img_train.astype('float32') / 255.0\n",
    "        x_img_test_normalize = x_img_test.astype('float32') / 255.0\n",
    "\n",
    "        #轉換label 為OneHot Encoding\n",
    "        y_label_train_OneHot = np_utils.to_categorical(y_label_train, num_classes=24)\n",
    "        y_label_test_OneHot = np_utils.to_categorical(y_label_test, num_classes=24)\n",
    "\n",
    "        #print: train data/test data\n",
    "        #print(\"(round:\",j,\", dataset:\",i,\")\")\n",
    "        #print(\"train data:\",'images:',x_img_train.shape,\n",
    "        #      \" labels:\",y_label_train.shape) \n",
    "        #print(\"test  data:\",'images:',x_img_test.shape ,\n",
    "        #      \" labels:\",y_label_test.shape)\n",
    "\n",
    "        train_history=model.fit(x_img_train_normalize, y_label_train_OneHot, \n",
    "                                validation_split=0.2, epochs=1, \n",
    "                                batch_size=64, verbose=1)\n",
    "        \n",
    "        save_train_history(train_history,'acc','val_acc')\n",
    "        save_train_history2(train_history,'loss','val_loss')\n",
    "               \n",
    "        score = model.evaluate(x_img_test_normalize, y_label_test_OneHot, verbose=0)\n",
    "        #arr_num[i]=x_img_train_normalize.shape[0]       \n",
    "        \n",
    "        arr_num[i]=x_img_train_normalize.shape[0]       \n",
    "        arr_accuracy[i]=score[1]\n",
    "        arr_loss[i]=score[0]\n",
    "    \n",
    "    average_accuracy=np.average(arr_accuracy, weights=arr_num)\n",
    "    print(\"(Average) Test Accuracy=\",average_accuracy)\n",
    "\n",
    "    average_loss=np.average(arr_loss, weights=arr_num)\n",
    "    print(\"(Average) Test Loss=\",average_loss)\n",
    "    print(\"\")\n",
    "    testaccList.append(average_accuracy)\n",
    "    teslossList.append(average_loss)\n",
    "        \n",
    "        #score = model.evaluate(x_img_train_normalize, y_label_train_OneHot, verbose=0)\n",
    "        #print('Test loss:', score[0])\n",
    "        #print('Test accuracy:', score[1])\n",
    "        \n",
    "        #arr_num[i]=x_img_train_normalize.shape[0]       \n",
    "        #arr_accuracy[i]=score[1]\n",
    "        #arr_loss[i]=score[0]\n",
    "tEnd = time.time() #計時結束\n",
    "print (\"It cost %f sec\" % (tEnd - tStart))#會自動做近位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def myshow_train_history(train_history,train,validation):\n",
    "    plt.plot(my_trainList)\n",
    "    plt.plot(my_validList)\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train','validation'], loc='upper left')\n",
    "    plt.show()\n",
    "def myshow_train_history2(train_history,train,validation):\n",
    "    plt.plot(my_trainList2)\n",
    "    plt.plot(my_validList2)\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train','validation'], loc='upper left')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXBTMzKzgomBmZgUXBTMzK/wEP64Z/Ob2oGwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e06082780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myshow_train_history(train_history,'acc','val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e070f1b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myshow_train_history2(train_history,'loss','val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e061182e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(testaccList)\n",
    "plt.title('Test Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Round')\n",
    "#plt.legend(['train','validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x/yxWF2RHThVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e058430b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(teslossList)\n",
    "plt.title('Test Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Round')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Average) MAX Test Accuracy= 0.541553241303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(testaccList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('my_model_.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
