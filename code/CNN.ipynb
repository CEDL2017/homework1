{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import misc\n",
    "import glob\n",
    "import os, os.path\n",
    "from tensorflow.contrib.data import Dataset, Iterator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_dir = './'\n",
    "frame_dir = file_dir + 'frames/'\n",
    "label_dir = file_dir + 'labels/'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import time\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.ndimage import filters\n",
    "import urllib\n",
    "from numpy import random\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_env= '.'\n",
    "frames_path=os.path.join(data_env,'frames');\n",
    "label_path=os.path.join(data_env,'labels');\n",
    "\n",
    "def load_data(start_path):\n",
    "    X = []\n",
    "    FA = []\n",
    "    ges = []\n",
    "    obj = []\n",
    "    head = []\n",
    "    print(start_path)\n",
    "    for dir_name in os.listdir(start_path): # 'office', 'lab', 'house'\n",
    "        loc_path = os.path.join(start_path, dir_name)\n",
    "        for num in os.listdir(loc_path): # '1', '2', '3'\n",
    "            loc_num_path = os.path.join(loc_path, num)\n",
    "            for pos in os.listdir(loc_num_path): # 'Lhand', 'Rhand', 'head'\n",
    "                loc_num_pos_path = os.path.join(loc_num_path, pos)\n",
    "                file_names = os.listdir(loc_num_pos_path)\n",
    "                file_names = sorted(file_names, key=lambda x: int(re.sub('\\D', '', x)))\n",
    "                file_names = [os.path.join(loc_num_pos_path,f) for f in file_names]\n",
    "                if pos[0] == 'L':\n",
    "                    X += file_names\n",
    "                    if 'train' in start_path:\n",
    "                        FA += np.load(os.path.join(label_path, dir_name, 'FA_left'+num+'.npy')).astype(int).tolist()\n",
    "                        ges += np.load(os.path.join(label_path, dir_name, 'ges_left'+num+'.npy')).astype(int).tolist()\n",
    "                        obj += np.load(os.path.join(label_path, dir_name, 'obj_left'+num+'.npy')).astype(int).tolist()\n",
    "                    elif 'test' in start_path:\n",
    "                        if 'lab' in loc_num_pos_path:\n",
    "                            offset = 4\n",
    "                        else:\n",
    "                            offset = 3\n",
    "                        FA += np.load(os.path.join(label_path, dir_name, 'FA_left'+str(int(num)+offset)+'.npy')).astype(int).tolist()\n",
    "                        ges += np.load(os.path.join(label_path, dir_name, 'ges_left'+str(int(num)+offset)+'.npy')).astype(int).tolist()\n",
    "                        obj += np.load(os.path.join(label_path, dir_name, 'obj_left'+str(int(num)+offset)+'.npy')).astype(int).tolist()\n",
    "                elif pos[0] == 'R':\n",
    "                    X += file_names\n",
    "                    if 'train' in start_path:\n",
    "                        FA += np.load(os.path.join(label_path, dir_name, 'FA_right'+num+'.npy')).astype(int).tolist()\n",
    "                        ges += np.load(os.path.join(label_path, dir_name, 'ges_right'+num+'.npy')).astype(int).tolist()\n",
    "                        obj += np.load(os.path.join(label_path, dir_name, 'obj_right'+num+'.npy')).astype(int).tolist()\n",
    "                    elif 'test' in start_path:\n",
    "                        if 'lab' in loc_num_pos_path:\n",
    "                            offset = 4\n",
    "                        else:\n",
    "                            offset = 3\n",
    "                        FA += np.load(os.path.join(label_path, dir_name, 'FA_right'+str(int(num)+offset)+'.npy')).astype(int).tolist()\n",
    "                        ges += np.load(os.path.join(label_path, dir_name, 'ges_right'+str(int(num)+offset)+'.npy')).astype(int).tolist()\n",
    "                        obj += np.load(os.path.join(label_path, dir_name, 'obj_right'+str(int(num)+offset)+'.npy')).astype(int).tolist()\n",
    "                else:\n",
    "                    head += file_names\n",
    "                    head += file_names\n",
    "    \n",
    "    assert len(X) == len(FA) == len(ges) == len(obj) == len(head)\n",
    "    return np.array([X, FA, ges, obj, head])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "#xt_head, xt_lhand, xt_rhand, yt = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Alex_cls:\n",
    "    def __init__(self, hps):\n",
    "        self.hps = hps\n",
    "        \n",
    "        self._build_inputs()\n",
    "        self._build_model()\n",
    "        self._get_saver()\n",
    "        \n",
    "    def _build_inputs(self):\n",
    "        self.img = tf.placeholder(tf.float32, [None, self.hps.h_input, self.hps.w_input, 3])\n",
    "        self.FA = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "        self.ges = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "        self.obj = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "        \n",
    "        # convert the label to one-hot encoding\n",
    "        self.one_hot_FA = tf.one_hot(self.FA, 2)\n",
    "        self.one_hot_ges = tf.one_hot(self.ges, 13)\n",
    "        self.one_hot_obj = tf.one_hot(self.obj, 24)\n",
    "    \n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            kernel = tf.get_variable('weights', initializer=tf.truncated_normal(shape=[3, 3, 3, 16]))\n",
    "            conv = tf.nn.conv2d(self.img, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.get_variable('biases', [kernel.shape[-1]], initializer=tf.constant_initializer(0.1))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "            # pool1\n",
    "            pool = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool')\n",
    "            # norm1\n",
    "            norm = tf.nn.lrn(pool, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm')\n",
    "\n",
    "        # conv2\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            kernel = tf.get_variable('weights', initializer=tf.truncated_normal(shape=[3, 3, 16, 32]))\n",
    "            conv = tf.nn.conv2d(norm, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.get_variable('biases', [kernel.shape[-1]], initializer=tf.constant_initializer(0.1))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "            # pool2\n",
    "            pool = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool')\n",
    "            # norm2\n",
    "            norm = tf.nn.lrn(pool, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm')\n",
    "\n",
    "        # conv3\n",
    "        with tf.variable_scope('conv3') as scope:\n",
    "            kernel = tf.get_variable('weights', initializer=tf.truncated_normal(shape=[3, 3, 32, 32]))\n",
    "            conv = tf.nn.conv2d(norm, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.get_variable('biases', [kernel.shape[-1]], initializer=tf.constant_initializer(0.1))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "            # pool3\n",
    "            pool = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool')\n",
    "            # norm3\n",
    "            norm = tf.nn.lrn(pool, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm')\n",
    "        \n",
    "        # conv4\n",
    "        with tf.variable_scope('conv4') as scope:\n",
    "            kernel = tf.get_variable('weights', initializer=tf.truncated_normal(shape=[3, 3, 32, 32]))\n",
    "            conv = tf.nn.conv2d(norm, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.get_variable('biases', [kernel.shape[-1]], initializer=tf.constant_initializer(0.1))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "            # pool4\n",
    "            pool = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool')\n",
    "            # norm4\n",
    "            norm = tf.nn.lrn(pool, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm')\n",
    "            \n",
    "        # local3 shared fcn layer\n",
    "        with tf.variable_scope('local3') as scope:\n",
    "            # Move everything into depth so we can perform a single matrix multiply.\n",
    "            reshape = tf.reshape(norm, [-1, 1024])\n",
    "            weights = tf.get_variable('weights', initializer=tf.truncated_normal(shape=[1024, 192]))\n",
    "            biases = tf.get_variable('biases', [192], initializer=tf.constant_initializer(0.1))\n",
    "            local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('softmax_linear_FA') as scope:\n",
    "            weights = tf.get_variable('weights', initializer=tf.truncated_normal(shape=[192, 2]))\n",
    "            biases = tf.get_variable('biases', [2], initializer=tf.constant_initializer(0.1))\n",
    "            softmax_linear_FA = tf.add(tf.matmul(local3, weights), biases, name=scope.name)\n",
    "            self.output_FA = tf.nn.softmax(softmax_linear_FA)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    labels=self.one_hot_FA, logits=softmax_linear_FA, name='cross_entropy')\n",
    "            cross_entropy_mean_FA = tf.reduce_mean(cross_entropy, name='cross_entropy_mean')\n",
    "\n",
    "        with tf.variable_scope('softmax_linear_ges') as scope:\n",
    "            weights = tf.get_variable('weights', initializer=tf.truncated_normal(shape=[192, 13]))\n",
    "            biases = tf.get_variable('biases', [13], initializer=tf.constant_initializer(0.1))\n",
    "            softmax_linear_ges = tf.add(tf.matmul(local3, weights), biases, name=scope.name)\n",
    "            self.output_ges = tf.nn.softmax(softmax_linear_ges)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    labels=self.one_hot_ges, logits=softmax_linear_ges, name='cross_entropy')\n",
    "            cross_entropy_mean_ges = tf.reduce_mean(cross_entropy, name='cross_entropy_mean')\n",
    "\n",
    "        with tf.variable_scope('softmax_linear_obj') as scope:\n",
    "            weights = tf.get_variable('weights', initializer=tf.truncated_normal(shape=[192, 24]))\n",
    "            biases = tf.get_variable('biases', [24], initializer=tf.constant_initializer(0.1))\n",
    "            softmax_linear_obj = tf.add(tf.matmul(local3, weights), biases, name=scope.name)\n",
    "            self.output_obj = tf.nn.softmax(softmax_linear_obj)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    labels=self.one_hot_obj, logits=softmax_linear_obj, name='cross_entropy')\n",
    "            cross_entropy_mean_obj = tf.reduce_mean(cross_entropy, name='cross_entropy_mean')\n",
    "\n",
    "        #calculate loss and training operation\n",
    "        self.loss = cross_entropy_mean_FA*2 + cross_entropy_mean_ges*13 + cross_entropy_mean_obj*24\n",
    "        \n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.hps.lr)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step)\n",
    "    \n",
    "    def build_data(self):\n",
    "        #width = 32\n",
    "        #height = 18\n",
    "        width = 128\n",
    "        height = 64\n",
    "        \n",
    "        # load dataset\n",
    "        X, FA, ges, obj, X_head = load_data(os.path.join('./frames/train'))\n",
    "        np.random.seed(0)\n",
    "        idx = np.random.choice(len(X), size=int(len(X)*0.3), replace=False).astype(int)\n",
    "        valid_idx = np.zeros(len(X), dtype=bool)\n",
    "        valid_idx[idx] = True\n",
    "        train_idx = np.ones(len(X), dtype=bool)\n",
    "        train_idx[idx] = False\n",
    "       \n",
    "        print('num X_train:',len(X))\n",
    "\n",
    "        X_train_path = tf.constant(X[train_idx])\n",
    "        X_train_FA = tf.constant(FA[train_idx])\n",
    "        X_train_ges = tf.constant(ges[train_idx])\n",
    "        X_train_obj = tf.constant(obj[train_idx])\n",
    "        X_train_head_path = tf.constant(X_head[train_idx])\n",
    "        X_valid_path = tf.constant(X[valid_idx])\n",
    "        X_valid_FA = tf.constant(FA[valid_idx])\n",
    "        X_valid_ges = tf.constant(ges[valid_idx])\n",
    "        X_valid_obj = tf.constant(obj[valid_idx])\n",
    "        X_valid_head_path = tf.constant(X_head[valid_idx])\n",
    "        self.num_train = X_train_path.shape[0].value\n",
    "        self.num_valid = X_valid_path.shape[0].value\n",
    "\n",
    "\n",
    "        # create TensorFlow Dataset objects\n",
    "        dataset = Dataset.from_tensor_slices((X_train_path, X_train_FA, X_train_ges, X_train_obj, X_train_head_path))\n",
    "        valid_dataset = Dataset.from_tensor_slices((X_valid_path, X_valid_FA, X_valid_ges, X_valid_obj, X_valid_head_path))\n",
    "        \n",
    "        \n",
    "        def data_generator(X_train_path, X_train_FA, X_train_ges, X_train_obj, X_train_head_path):\n",
    "            # read the img from file\n",
    "            img_file = tf.read_file(X_train_path)\n",
    "            img = tf.image.decode_image(img_file, channels=3)\n",
    "            img = tf.image.convert_image_dtype(img, tf.float64)\n",
    "            img.set_shape([1080, 1920, 3])\n",
    "            img = tf.image.resize_images(img, size=[self.hps.h_input,self.hps.w_input])\n",
    "\n",
    "            # read the img from file\n",
    "            img_file = tf.read_file(X_train_head_path)\n",
    "            img_head = tf.image.decode_image(img_file, channels=3)\n",
    "            img_head = tf.image.convert_image_dtype(img_head, tf.float64)\n",
    "            img_head.set_shape([1080, 1920, 3])\n",
    "            img_head = tf.image.resize_images(img_head, size=[self.hps.h_input,self.hps.w_input])\n",
    "\n",
    "\n",
    "            return img, X_train_FA, X_train_ges, X_train_obj, img_head\n",
    "\n",
    "        dataset = dataset.map(data_generator,num_threads=4, output_buffer_size=10*self.hps.batch_size)\n",
    "        dataset = dataset.batch(self.hps.batch_size)\n",
    "        \n",
    "        valid_dataset = valid_dataset.map(data_generator,num_threads=4, output_buffer_size=3*self.hps.batch_size)\n",
    "        valid_dataset = valid_dataset.batch(self.hps.batch_size)\n",
    "\n",
    "        # # create TensorFlow Iterator object\n",
    "        iterator = Iterator.from_structure(dataset.output_types,\n",
    "                                           dataset.output_shapes)\n",
    "        self.next_element = iterator.get_next()\n",
    "\n",
    "        valid_iterator = Iterator.from_structure(valid_dataset.output_types,\n",
    "                                           valid_dataset.output_shapes)\n",
    "        self.valid_next_element = valid_iterator.get_next()\n",
    "        \n",
    "\n",
    "        # # create two initialization ops to switch between the datasets\n",
    "        self.training_init_op = iterator.make_initializer(dataset)\n",
    "        self.validating_init_op = valid_iterator.make_initializer(valid_dataset)\n",
    "    def build_test_data(self):\n",
    "        width = 128\n",
    "        height = 64\n",
    "        X_test_path, X_test_FA, X_test_ges, X_test_obj, X_test_head_path = load_data(os.path.join('./frames/test'))\n",
    "        print('num X_test:',len(X_test_path))\n",
    "        \n",
    "        self.num_test = len(X_test_path)\n",
    "        test_dataset = Dataset.from_tensor_slices((X_test_path, X_test_FA, X_test_ges, X_test_obj, X_test_head_path))\n",
    "        \n",
    "        def data_generator(X_train_path, X_train_FA, X_train_ges, X_train_obj, X_train_head_path):\n",
    "            # read the img from file\n",
    "            img_file = tf.read_file(X_train_path)\n",
    "            img = tf.image.decode_image(img_file, channels=3)\n",
    "            img = tf.image.convert_image_dtype(img, tf.float64)\n",
    "            img.set_shape([1080, 1920, 3])\n",
    "            img = tf.image.resize_images(img, size=[self.hps.h_input,self.hps.w_input])\n",
    "\n",
    "            # read the img from file\n",
    "            img_file = tf.read_file(X_train_head_path)\n",
    "            img_head = tf.image.decode_image(img_file, channels=3)\n",
    "            img_head = tf.image.convert_image_dtype(img_head, tf.float64)\n",
    "            img_head.set_shape([1080, 1920, 3])\n",
    "            img_head = tf.image.resize_images(img_head, size=[self.hps.h_input,self.hps.w_input])\n",
    "\n",
    "\n",
    "            return img, X_train_FA, X_train_ges, X_train_obj, img_head\n",
    "        \n",
    "        test_dataset = test_dataset.map(data_generator, num_threads=4, output_buffer_size=self.hps.batch_size)\n",
    "        test_dataset = test_dataset.batch(1)\n",
    "        \n",
    "        test_iterator = Iterator.from_structure(test_dataset.output_types,\n",
    "                                            test_dataset.output_shapes)\n",
    "        self.test_next_element = test_iterator.get_next()\n",
    "        \n",
    "        self.testing_init_op = test_iterator.make_initializer(test_dataset)\n",
    "        \n",
    "    def _get_saver(self):\n",
    "        #self.saver = tf.train.Saver(var_list=tf.trainable_variables())\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def save_model(self, sess, global_step):\n",
    "        self.saver.save(sess, self.hps.ckpt+'cedlCNN', global_step)\n",
    "        print(\"saved model with step %d\" %(global_step))\n",
    "        \n",
    "    def load_model(self, sess):\n",
    "        ckpt = tf.train.get_checkpoint_state(self.hps.ckpt)\n",
    "        tf.logging.info('Loading model %s.', ckpt.model_checkpoint_path)\n",
    "        #print('Loading model %s.', ckpt.model_checkpoint_path)\n",
    "        self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    def train(self):\n",
    "        self.build_data()\n",
    "        with tf.Session() as sess:\n",
    "            ckpt = tf.train.get_checkpoint_state(self.hps.ckpt)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                self.load_model(sess)\n",
    "            else:\n",
    "                #no checkpoint\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            best_acc = 0\n",
    "            for epoch in range(self.hps.training_epochs):\n",
    "                sess.run(self.training_init_op)\n",
    "                loss = []\n",
    "                while True:\n",
    "                    try:\n",
    "                        X_train, X_train_FA, X_train_ges, X_train_obj, X_train_head = sess.run(self.next_element)\n",
    "\n",
    "                        step = sess.run(model.global_step)\n",
    "                        feed_dict = {\n",
    "                            self.img: X_train,\n",
    "                            self.FA: X_train_FA,\n",
    "                            self.ges: X_train_ges,\n",
    "                            self.obj: X_train_obj,\n",
    "                        }\n",
    "                        _, l, step = sess.run([self.train_op, self.loss, self.global_step], feed_dict=feed_dict)\n",
    "                        loss.append(l)\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        print(\"epoch %d with total training loss %f\" %(epoch, sum(loss)))\n",
    "                        break\n",
    "                FA_accs = []\n",
    "                ges_accs = []\n",
    "                obj_accs = []\n",
    "                valid_losses = []\n",
    "                total = 0\n",
    "                FA_acc = 0\n",
    "                ges_acc = 0\n",
    "                obj_acc = 0\n",
    "                losses = 0\n",
    "                sess.run(self.validating_init_op)\n",
    "                while True:\n",
    "                    try:\n",
    "                        X_train, X_train_FA, X_train_ges, X_train_obj, X_train_head = sess.run(self.valid_next_element)\n",
    "\n",
    "                        feed_dict = {\n",
    "                            self.img: X_train,\n",
    "                            self.FA: X_train_FA,\n",
    "                            self.ges: X_train_ges, \n",
    "                            self.obj:X_train_obj\n",
    "                        }\n",
    "                        loss, output_FA, output_ges, output_obj, step = sess.run(\n",
    "                                [self.loss, self.output_FA, self.output_ges, self.output_obj, self.global_step], feed_dict=feed_dict)\n",
    "                        FA_acc += np.sum(np.argmax(output_FA, axis=1)==X_train_FA.astype(int))\n",
    "                        ges_acc += np.sum(np.argmax(output_ges, axis=1)==X_train_ges.astype(int))\n",
    "                        obj_acc += np.sum(np.argmax(output_obj, axis=1)==X_train_obj.astype(int))\n",
    "                        losses += loss*len(X_train)\n",
    "                        total += len(X_train)\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        losses /= self.num_valid\n",
    "                        valid_losses.append(losses)\n",
    "                        FA_acc = FA_acc/total\n",
    "                        ges_acc = ges_acc/total\n",
    "                        obj_acc = obj_acc/total\n",
    "                        FA_accs.append(FA_acc)\n",
    "                        ges_accs.append(ges_acc)\n",
    "                        obj_accs.append(obj_acc)\n",
    "                        mean_acc = np.mean([FA_acc, ges_acc, obj_acc])\n",
    "                        if mean_acc > best_acc:\n",
    "                            best_acc = mean_acc                \n",
    "                            model.save_model(sess, step)\n",
    "\n",
    "                        print('INFO:root:Epoch[%d] loss=%f' %(epoch, losses))\n",
    "                        print('INFO:root:Epoch[%d] FA Validation-accuracy=%f' %(step, FA_acc))\n",
    "                        print('INFO:root:Epoch[%d] ges Validation-accuracy=%f' %(step, ges_acc))\n",
    "                        print('INFO:root:Epoch[%d] obj Validation-accuracy=%f' %(step, obj_acc))\n",
    "                        print()\n",
    "                        break\n",
    "                FA_accs = np.array(FA_accs)\n",
    "                ges_accs = np.array(ges_accs)\n",
    "                obj_asss = np.array(obj_accs)\n",
    "                valid_losses = np.array(valid_losses)\n",
    "                np.save('FA_acc.npy', FA_accs)\n",
    "                np.save('ges_acc.npy', ges_accs)\n",
    "                np.save('obj_acc.npy', obj_accs)\n",
    "                np.save('valid_loss.npy', valid_losses)\n",
    "\n",
    "                \n",
    "    def test(self):\n",
    "        self.build_test_data()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            ckpt = tf.train.get_checkpoint_state(self.hps.ckpt)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                self.load_model(sess)\n",
    "            else:\n",
    "                print(\"no chekpoint found for testing\")\n",
    "                return\n",
    "                #sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "            FA_accs = []\n",
    "            ges_accs = []\n",
    "            obj_accs = []\n",
    "            #valid_losses = []\n",
    "            best_acc = 0\n",
    "            total = 0\n",
    "            FA_acc = 0\n",
    "            ges_acc = 0\n",
    "            obj_acc = 0\n",
    "            #losses = 0\n",
    "            sess.run(self.testing_init_op)\n",
    "            while True:\n",
    "                try:\n",
    "                    X_train, X_train_FA, X_train_ges, X_train_obj, X_train_head = sess.run(self.test_next_element)\n",
    "                    feed_dict = {\n",
    "                        self.img: X_train,\n",
    "                    }\n",
    "                    \n",
    "                    \n",
    "                    output_FA, output_ges, output_obj, step = sess.run(\n",
    "                            [self.output_FA, self.output_ges, self.output_obj, self.global_step], feed_dict=feed_dict)\n",
    "                    FA_acc += np.sum(np.argmax(output_FA, axis=1)==X_train_FA.astype(int))\n",
    "                    ges_acc += np.sum(np.argmax(output_ges, axis=1)==X_train_ges.astype(int))\n",
    "                    obj_acc += np.sum(np.argmax(output_obj, axis=1)==X_train_obj.astype(int))\n",
    "                    #losses += loss*len(X_train)\n",
    "                    total += len(X_train)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    #losses /= self.num_test\n",
    "                    #valid_losses.append(losses)\n",
    "                    FA_acc = FA_acc/total\n",
    "                    ges_acc = ges_acc/total\n",
    "                    obj_acc = obj_acc/total\n",
    "                    FA_accs.append(FA_acc)\n",
    "                    ges_accs.append(ges_acc)\n",
    "                    obj_accs.append(obj_acc)\n",
    "                    mean_acc = np.mean([FA_acc, ges_acc, obj_acc])\n",
    "                    if mean_acc > best_acc:\n",
    "                        best_acc = mean_acc                \n",
    "                        model.save_model(sess, step)\n",
    "\n",
    "                    #print('INFO:root:Testing loss=%f' %(step, losses))\n",
    "                    print('INFO:root:Testing FA-accuracy=%f' %(FA_acc))\n",
    "                    print('INFO:root:Testing ges-accuracy=%f' %(ges_acc))\n",
    "                    print('INFO:root:Testing obj-accuracy=%f' %(obj_acc))\n",
    "                    print()\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_hparams():\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "          batch_size=64,\n",
    "          lr=1e-4,\n",
    "          training_epochs=10,\n",
    "          w_input = 64,\n",
    "\n",
    "          h_input = 128,\n",
    "          ckpt='./model_ckpt/')\n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "hps = get_hparams()\n",
    "model = Alex_cls(hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./frames/train\n",
      "num X_train: 14992\n",
      "epoch 0 with total training loss 24309492.000000\n",
      "saved model with step 164\n",
      "INFO:root:Epoch[0] loss=60881.778413\n",
      "INFO:root:Epoch[164] FA Validation-accuracy=0.495664\n",
      "INFO:root:Epoch[164] ges Validation-accuracy=0.228152\n",
      "INFO:root:Epoch[164] obj Validation-accuracy=0.239715\n",
      "\n",
      "epoch 1 with total training loss 8392971.000000\n",
      "INFO:root:Epoch[1] loss=41066.290777\n",
      "INFO:root:Epoch[328] FA Validation-accuracy=0.480987\n",
      "INFO:root:Epoch[328] ges Validation-accuracy=0.268401\n",
      "INFO:root:Epoch[328] obj Validation-accuracy=0.213920\n",
      "\n",
      "epoch 2 with total training loss 5764279.500000\n",
      "INFO:root:Epoch[2] loss=30439.293479\n",
      "INFO:root:Epoch[492] FA Validation-accuracy=0.470091\n",
      "INFO:root:Epoch[492] ges Validation-accuracy=0.248165\n",
      "INFO:root:Epoch[492] obj Validation-accuracy=0.236602\n",
      "\n",
      "epoch 3 with total training loss 4296810.000000\n",
      "INFO:root:Epoch[3] loss=24275.771281\n",
      "INFO:root:Epoch[656] FA Validation-accuracy=0.466978\n",
      "INFO:root:Epoch[656] ges Validation-accuracy=0.239715\n",
      "INFO:root:Epoch[656] obj Validation-accuracy=0.241717\n",
      "\n",
      "epoch 4 with total training loss 3492549.000000\n",
      "saved model with step 820\n",
      "INFO:root:Epoch[4] loss=20918.691893\n",
      "INFO:root:Epoch[820] FA Validation-accuracy=0.468757\n",
      "INFO:root:Epoch[820] ges Validation-accuracy=0.255948\n",
      "INFO:root:Epoch[820] obj Validation-accuracy=0.248833\n",
      "\n",
      "epoch 5 with total training loss 3103361.750000\n",
      "saved model with step 984\n",
      "INFO:root:Epoch[5] loss=18813.996310\n",
      "INFO:root:Epoch[984] FA Validation-accuracy=0.474761\n",
      "INFO:root:Epoch[984] ges Validation-accuracy=0.268624\n",
      "INFO:root:Epoch[984] obj Validation-accuracy=0.261285\n",
      "\n",
      "epoch 6 with total training loss 2839652.000000\n",
      "saved model with step 1148\n",
      "INFO:root:Epoch[6] loss=17220.288041\n",
      "INFO:root:Epoch[1148] FA Validation-accuracy=0.477874\n",
      "INFO:root:Epoch[1148] ges Validation-accuracy=0.283078\n",
      "INFO:root:Epoch[1148] obj Validation-accuracy=0.277296\n",
      "\n",
      "epoch 7 with total training loss 2627404.000000\n",
      "saved model with step 1312\n",
      "INFO:root:Epoch[7] loss=16021.554400\n",
      "INFO:root:Epoch[1312] FA Validation-accuracy=0.481210\n",
      "INFO:root:Epoch[1312] ges Validation-accuracy=0.290416\n",
      "INFO:root:Epoch[1312] obj Validation-accuracy=0.284189\n",
      "\n",
      "epoch 8 with total training loss 2453436.000000\n",
      "saved model with step 1476\n",
      "INFO:root:Epoch[8] loss=15012.201302\n",
      "INFO:root:Epoch[1476] FA Validation-accuracy=0.478986\n",
      "INFO:root:Epoch[1476] ges Validation-accuracy=0.294196\n",
      "INFO:root:Epoch[1476] obj Validation-accuracy=0.285301\n",
      "\n",
      "epoch 9 with total training loss 2306652.000000\n",
      "saved model with step 1640\n",
      "INFO:root:Epoch[9] loss=14145.430121\n",
      "INFO:root:Epoch[1640] FA Validation-accuracy=0.480543\n",
      "INFO:root:Epoch[1640] ges Validation-accuracy=0.306427\n",
      "INFO:root:Epoch[1640] obj Validation-accuracy=0.291528\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./frames/test\n",
      "num X_test: 12776\n",
      "INFO:tensorflow:Loading model ./model_ckpt/cedlCNN-1640.\n",
      "INFO:tensorflow:Restoring parameters from ./model_ckpt/cedlCNN-1640\n",
      "saved model with step 1640\n",
      "INFO:root:Testing FA-accuracy=0.485520\n",
      "INFO:root:Testing ges-accuracy=0.238181\n",
      "INFO:root:Testing obj-accuracy=0.274186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.test()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:katy]",
   "language": "python",
   "name": "conda-env-katy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
