{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_dir = './'\n",
    "frame_dir = file_dir + 'frames/'\n",
    "label_dir = file_dir + 'labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import misc\n",
    "import glob\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import time\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.ndimage import filters\n",
    "import urllib\n",
    "from numpy import random\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(batch_size):\n",
    "    #train_x_head = []\n",
    "    train_x_lhand= []\n",
    "    #train_x_rhand= []\n",
    "    train_y = []\n",
    "    #frames train house 2\n",
    "    random.seed(9001)\n",
    "    place = ['house/', 'lab/', 'office/'][random.randint(0, 2)]\n",
    "    if place is 'lab/':\n",
    "        num = random.randint(1, 4)\n",
    "        \n",
    "        fdir = frame_dir+'train/'+place+str(num)+'/'\n",
    "        ldir = label_dir+place\n",
    "        #load labels\n",
    "        FA_left = np.eye(2)[np.load(ldir+'FA_left'+str(num)+'.npy').astype(int)]\n",
    "        \n",
    "\n",
    "        ges_left = np.eye(13)[np.load(ldir+'ges_left'+str(num)+'.npy').astype(int)]\n",
    "        \n",
    "\n",
    "        obj_left = np.eye(24)[np.load(ldir+'obj_left'+str(num)+'.npy').astype(int)]\n",
    "        \n",
    "\n",
    "        train_y.extend(concatenate((FA_left, ges_left, obj_left), axis=1))\n",
    "        \n",
    "        \n",
    "        file_count = len(os.listdir(fdir+'head/'))\n",
    "        index = random.choice(list(range(file_count)), batch_size)\n",
    "        train_y = [train_y[i] for i in index]\n",
    "        \n",
    "        #load frames\n",
    "        for i in index+1:\n",
    "            \n",
    "            train_x_lhand.append(misc.imresize(misc.imread(fdir+'Lhand/Image'+str(i)+'.png'), [135,240]))\n",
    " \n",
    "    else:\n",
    "        num = random.randint(1, 3)\n",
    "        \n",
    "        fdir = frame_dir+'train/'+place+str(num)+'/'\n",
    "        ldir = label_dir+place\n",
    "        #load labels\n",
    "        FA_left = np.eye(2)[np.load(ldir+'FA_left'+str(num)+'.npy').astype(int)]\n",
    "        #FA_right = np.eye(2)[np.load(ldir+'FA_right'+str(num)+'.npy').astype(int)]\n",
    "\n",
    "        ges_left = np.eye(13)[np.load(ldir+'ges_left'+str(num)+'.npy').astype(int)]\n",
    "        #ges_right = np.eye(13)[np.load(ldir+'ges_right'+str(num)+'.npy').astype(int)]\n",
    "\n",
    "        obj_left = np.eye(24)[np.load(ldir+'obj_left'+str(num)+'.npy').astype(int)]\n",
    "\n",
    "        train_y.extend(concatenate((FA_left, ges_left, obj_left), axis=1))\n",
    "        \n",
    "        \n",
    "        file_count = len(os.listdir(fdir+'head/'))\n",
    "        index = random.choice(list(range(file_count)), batch_size)\n",
    "        train_y = [train_y[i] for i in index]\n",
    "        \n",
    "        #load frames\n",
    "        for i in index+1:\n",
    "            \n",
    "            train_x_lhand.append(misc.imresize(misc.imread(fdir+'Lhand/Image'+str(i)+'.png'), [135,240]))\n",
    " \n",
    "                            \n",
    "\n",
    "    return [train_x_lhand, train_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_data():\n",
    "    test_x_dir = []\n",
    "    #train_x_rhand= []\n",
    "    test_y = []\n",
    "    #frames train house 2\n",
    "    for place in ['house/', 'lab/', 'office/']:\n",
    "        if place is 'lab/':\n",
    "            for num in range(1,5):\n",
    "                \n",
    "                fdir = frame_dir+'test/'+place+str(num)+'/'\n",
    "                ldir = label_dir+place\n",
    "                #load labels\n",
    "                FA_left = np.eye(2)[np.load(ldir+'FA_left'+str(3+num)+'.npy').astype(int)]\n",
    "                ges_left = np.eye(13)[np.load(ldir+'ges_left'+str(3+num)+'.npy').astype(int)]\n",
    "                obj_left = np.eye(24)[np.load(ldir+'obj_left'+str(3+num)+'.npy').astype(int)]\n",
    "                test_y.extend(concatenate((FA_left, ges_left, obj_left), axis=1))\n",
    "\n",
    "                file_count = len(os.listdir(fdir+'head/'))\n",
    "\n",
    "                #load frames\n",
    "                for i in range(file_count):\n",
    "                    test_x_dir.append(fdir+'Lhand/Image'+str(i+1)+'.png')\n",
    "               \n",
    "        else:\n",
    "             for num in range(1,4):\n",
    "\n",
    "                fdir = frame_dir+'test/'+place+str(num)+'/'\n",
    "                ldir = label_dir+place\n",
    "                #load labels\n",
    "                FA_left = np.eye(2)[np.load(ldir+'FA_left'+str(3+num)+'.npy').astype(int)]\n",
    "                ges_left = np.eye(13)[np.load(ldir+'ges_left'+str(3+num)+'.npy').astype(int)]\n",
    "                obj_left = np.eye(24)[np.load(ldir+'obj_left'+str(3+num)+'.npy').astype(int)]\n",
    "                test_y.extend(concatenate((FA_left, ges_left, obj_left), axis=1))\n",
    "\n",
    "                file_count = len(os.listdir(fdir+'head/'))\n",
    "\n",
    "                #load frames\n",
    "                for i in range(file_count):\n",
    "                    test_x_dir.append(fdir+'Lhand/Image'+str(i+1)+'.png')\n",
    "    return [test_x_dir, test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "#xt_head, xt_lhand, xt_rhand, yt = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Iter 100, Minibatch Loss= 127651.03, Training Accuracy FA= 0.75000, Training Accuracy GES= 0.30000, Training Accuracy OBJ= 0.10000\n",
      "Iter 200, Minibatch Loss= 74700.05, Training Accuracy FA= 0.85000, Training Accuracy GES= 0.30000, Training Accuracy OBJ= 0.35000\n",
      "Iter 300, Minibatch Loss= 56194.27, Training Accuracy FA= 0.85000, Training Accuracy GES= 0.45000, Training Accuracy OBJ= 0.45000\n",
      "Iter 400, Minibatch Loss= 37665.08, Training Accuracy FA= 0.80000, Training Accuracy GES= 0.45000, Training Accuracy OBJ= 0.55000\n",
      "Iter 500, Minibatch Loss= 30029.28, Training Accuracy FA= 0.70000, Training Accuracy GES= 0.60000, Training Accuracy OBJ= 0.60000\n",
      "Iter 600, Minibatch Loss= 24170.48, Training Accuracy FA= 0.70000, Training Accuracy GES= 0.60000, Training Accuracy OBJ= 0.65000\n",
      "Iter 700, Minibatch Loss= 15219.27, Training Accuracy FA= 0.70000, Training Accuracy GES= 0.80000, Training Accuracy OBJ= 0.85000\n",
      "Iter 800, Minibatch Loss= 12286.51, Training Accuracy FA= 0.75000, Training Accuracy GES= 0.80000, Training Accuracy OBJ= 0.85000\n",
      "Iter 900, Minibatch Loss= 11053.92, Training Accuracy FA= 0.75000, Training Accuracy GES= 0.80000, Training Accuracy OBJ= 0.90000\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-67b469593977>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mall_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy_fa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_ges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_obj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfromx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtox\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mavg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_acc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing Accuracy FA:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"{:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"Testing Accuracy GES:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"{:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"Testing Accuracy FA:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"{:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "\n",
    "# hyperparameters\n",
    "learning_rate = 0.0001\n",
    "training_iters = 1000\n",
    "batch_size = 20\n",
    "display_step = 100\n",
    "\n",
    "# other parameters\n",
    "w_input = 135\n",
    "h_input = 240\n",
    "d_input = 3\n",
    "n_classes = 2+13+24\n",
    "dropout = 0.8 \n",
    "\n",
    "# placeholders\n",
    "x = tf.placeholder(tf.float32, [None, w_input, h_input, d_input])\n",
    "#x_lhand = tf.placeholder(tf.float32, [batch_size, w_input, h_input, d_input])\n",
    "#x_rhand = tf.placeholder(tf.float32, [batch_size, w_input, h_input, d_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# convolution\n",
    "def conv2d(name, l_input, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], padding='SAME'),b), name=name)\n",
    "\n",
    "# max pooling\n",
    "def max_pool(name, l_input, k):\n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME', name=name)\n",
    "\n",
    "# batch norm\n",
    "def norm(name, l_input, lsize=4):\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n",
    "\n",
    "# network \n",
    "def alex_net(_X, _weights, _biases, _dropout):\n",
    "\n",
    "    # conv 1\n",
    "    conv1 = conv2d('conv1', _X, _weights['wc1'], _biases['bc1'])\n",
    "    pool1 = max_pool('pool1', conv1, k=2)\n",
    "    norm1 = norm('norm1', pool1, lsize=4)\n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)\n",
    "\n",
    "    # conv 2\n",
    "    conv2 = conv2d('conv2', norm1, _weights['wc2'], _biases['bc2'])\n",
    "    pool2 = max_pool('pool2', conv2, k=2)\n",
    "    norm2 = norm('norm2', pool2, lsize=4)\n",
    "    norm2 = tf.nn.dropout(norm2, _dropout)\n",
    "\n",
    "    # conv 3\n",
    "    conv3 = conv2d('conv3', norm2, _weights['wc3'], _biases['bc3'])\n",
    "    pool3 = max_pool('pool3', conv3, k=2)\n",
    "    norm3 = norm('norm3', pool3, lsize=4)\n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)\n",
    "\n",
    "    # conv 4\n",
    "    conv3 = conv2d('conv3', norm2, _weights['wc3'], _biases['bc3'])\n",
    "    pool3 = max_pool('pool3', conv3, k=2)\n",
    "    norm3 = norm('norm3', pool3, lsize=4)\n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)\n",
    "\n",
    "#FA\n",
    "    # fc 1\n",
    "    dense1 = tf.reshape(norm3, [-1, _weights['wd1'].get_shape().as_list()[0]]) \n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1') \n",
    "    # fc 2\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') # Relu activation\n",
    "\n",
    "    # output layer\n",
    "    out1 = tf.matmul(dense2, _weights['out']) + _biases['out']\n",
    "    \n",
    "#ges\n",
    "    dense1 = tf.reshape(norm3, [-1, _weights['wd12'].get_shape().as_list()[0]]) \n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd12']) + _biases['bd12'], name='fc12') \n",
    "    # fc 2\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd22']) + _biases['bd22'], name='fc22') # Relu activation\n",
    "\n",
    "    # output layer\n",
    "    out2 = tf.matmul(dense2, _weights['out2']) + _biases['out2']\n",
    "#obj\n",
    "    dense1 = tf.reshape(norm3, [-1, _weights['wd13'].get_shape().as_list()[0]]) \n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd13']) + _biases['bd13'], name='fc13') \n",
    "    # fc 2\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd23']) + _biases['bd23'], name='fc23') # Relu activation\n",
    "\n",
    "    # output layer\n",
    "    out3 = tf.matmul(dense2, _weights['out3']) + _biases['out3']\n",
    "    \n",
    "    out = tf.concat([out1, out2, out3], 1)\n",
    "    \n",
    "    return out\n",
    "\n",
    "# weights and biases for the model\n",
    "weights_head = {\n",
    "    'wc1': tf.Variable(tf.random_normal([3, 3, 3, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wc3': tf.Variable(tf.random_normal([5, 5, 64, 128])),\n",
    "    'wd1': tf.Variable(tf.random_normal([65280, 256])),\n",
    "    'wd2': tf.Variable(tf.random_normal([256, 128])),\n",
    "    'out': tf.Variable(tf.random_normal([128, 2])), \n",
    "    \n",
    "    'wd12': tf.Variable(tf.random_normal([65280, 256])),\n",
    "    'wd22': tf.Variable(tf.random_normal([256, 128])),\n",
    "    'out2': tf.Variable(tf.random_normal([128, 13])),\n",
    "    \n",
    "    'wd13': tf.Variable(tf.random_normal([65280, 256])),\n",
    "    'wd23': tf.Variable(tf.random_normal([256, 128])),\n",
    "    'out3': tf.Variable(tf.random_normal([128, 24]))\n",
    "}\n",
    "biases_head = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bc3': tf.Variable(tf.random_normal([128])),\n",
    "    \n",
    "    'bd1': tf.Variable(tf.random_normal([256])),\n",
    "    'bd2': tf.Variable(tf.random_normal([128])),\n",
    "    'out': tf.Variable(tf.random_normal([2])),\n",
    "    \n",
    "    'bd12': tf.Variable(tf.random_normal([256])),\n",
    "    'bd22': tf.Variable(tf.random_normal([128])),\n",
    "    'out2': tf.Variable(tf.random_normal([13])),\n",
    "    \n",
    "    'bd13': tf.Variable(tf.random_normal([256])),\n",
    "    'bd23': tf.Variable(tf.random_normal([128])),\n",
    "    'out3': tf.Variable(tf.random_normal([24]))\n",
    "}\n",
    "\n",
    "\n",
    "# network\n",
    "pred = alex_net(x, weights_head, biases_head, keep_prob)\n",
    "\n",
    "\n",
    "def pred_net(x1, x2, _weights, _biases, _dropout):\n",
    "    x = tf.concat([x1, x2], axis = 0)\n",
    "    print(x)\n",
    "    # fc 1\n",
    "    dense1 = tf.reshape(x, [-1, _weights['wd1'].get_shape().as_list()[0]]) \n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1') \n",
    "    # fc 2\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') # Relu activation\n",
    "\n",
    "    # output layer\n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# loss function and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred[:,0:2], labels = y[:,0:2])) +\\\n",
    "tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred[:,2:15], labels = y[:,2:15]))+\\\n",
    "tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred[:,15:], labels = y[:,15:]))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# prediction\n",
    "correct_fa = tf.equal(tf.argmax(pred[:,0:2],1), tf.argmax(y[:,0:2],1))\n",
    "correct_ges = tf.equal(tf.argmax(pred[:,2:15],1), tf.argmax(y[:,2:15],1))\n",
    "correct_obj = tf.equal(tf.argmax(pred[:,15:],1), tf.argmax(y[:,15:],1))\n",
    "#correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy_fa = tf.reduce_mean(tf.cast(correct_fa, tf.float32))\n",
    "accuracy_ges = tf.reduce_mean(tf.cast(correct_ges, tf.float32))\n",
    "accuracy_obj = tf.reduce_mean(tf.cast(correct_obj, tf.float32))\n",
    "\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# initial\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# start training\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    print(\"start training\")\n",
    "    #batch_size = 10\n",
    "    # Keep training until reach max iterations\n",
    "    while step < training_iters:\n",
    "        xt, yt = load_data(batch_size)\n",
    "        \n",
    "        sess.run(optimizer, feed_dict={x:xt, y: yt, keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # accuracy\n",
    "            acc_fa, acc_ges, acc_obj = sess.run([accuracy_fa, accuracy_ges, accuracy_obj], feed_dict={x: xt, y: yt, keep_prob: 1.})\n",
    "            # loss\n",
    "            loss = sess.run(cost, feed_dict={x: xt,y: yt, keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step) + \", Minibatch Loss= \" + \"{:.2f}\".format(loss)+ \", Training Accuracy FA= \" \n",
    "                  + \"{:.5f}\".format(acc_fa)+ \", Training Accuracy GES= \" \n",
    "                  + \"{:.5f}\".format(acc_ges)+ \", Training Accuracy OBJ= \" \n",
    "                  + \"{:.5f}\".format(acc_obj))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    # test acc\n",
    "    xtest_dir, ytest = test_data()\n",
    "    test_size = 20\n",
    "    all_acc = [0, 0, 0]\n",
    "    for i in range(int(len(xtest_dir)/test_size)):\n",
    "        #read imgs from that\n",
    "        xtest = []\n",
    "        for j in range(test_size):\n",
    "            xtest.append(misc.imresize(misc.imread(xtest_dir[i*test_size+j]), [135,240]))\n",
    "\n",
    "        fromx = i*test_size\n",
    "        tox = (i+1)*test_size\n",
    "        all_acc += sess.run([accuracy_fa, accuracy_ges, accuracy_obj], feed_dict={x: xtest, y: ytest[fromx:tox], keep_prob: 1.})\n",
    "\n",
    "    avg_acc = [all_acc[0]/(int(len(xtest_dir)/test_size)), all_acc[1]/(int(len(xtest_dir)/test_size)),all_acc[2]/(int(len(xtest_dir)/test_size))]\n",
    "    print(\"Testing Accuracy FA:\"+\"{:.5f}\".format(avg_acc[0])+\"Testing Accuracy GES:\"+\"{:.5f}\".format(avg_acc[1])+\"Testing Accuracy FA:\"+\"{:.5f}\".format(avg_acc[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:katy]",
   "language": "python",
   "name": "conda-env-katy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
