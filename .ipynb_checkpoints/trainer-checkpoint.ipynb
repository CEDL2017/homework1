{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "import inception_preprocessing\n",
    "from inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6096\n"
     ]
    }
   ],
   "source": [
    "# place = ['house', 'lab', 'office']\n",
    "place = ['house']\n",
    "num = ['1', '2', '3']\n",
    "kind = ['Lhand', 'Rhand']\n",
    "img_num = 0\n",
    "\n",
    "for p in place:\n",
    "    for n in num:\n",
    "        for k in kind:\n",
    "            path = '../frames/train/' + p + '/' + n + '/' + k\n",
    "            png_list = os.listdir(path)\n",
    "            img_num += len(png_list)\n",
    "            for pic in png_list:\n",
    "                npic = pic.replace('Image', '')\n",
    "                npic = npic.replace('.png', '')\n",
    "                if int(npic) < 100:\n",
    "                    npic = 'Image' + npic.zfill(3) + '.png'\n",
    "                    os.rename(path + '/' + pic, path + '/' + npic)\n",
    "\n",
    "# for k in kind:\n",
    "#     path = '../frames/train/' + 'lab' + '/' + '4' + '/' + k\n",
    "#     png_list = os.listdir(path)\n",
    "#     img_num += len(png_list)\n",
    "#     for pic in png_list:\n",
    "#         npic = pic.replace('Image', '')\n",
    "#         npic = npic.replace('.png', '')\n",
    "#         if int(npic) < 100:\n",
    "#             npic = 'Image' + npic.zfill(3) + '.png'\n",
    "#             os.rename(path + '/' + pic, path + '/' + npic)\n",
    "\n",
    "print(img_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# place = ['house', 'lab', 'office']\n",
    "num = ['1', '2', '3']\n",
    "kind = ['left', 'right']\n",
    "\n",
    "path = '../labels/'\n",
    "\n",
    "label = np.array([])\n",
    "\n",
    "for p in place:\n",
    "    lab_list = os.listdir(path + p)\n",
    "    for lab in lab_list:\n",
    "        ali = lab\n",
    "        if 'obj' in ali:\n",
    "            ali = ali.replace('.npy', '')\n",
    "            for k in kind:\n",
    "                if k in ali:\n",
    "                    for n in num:\n",
    "                        if n in ali:\n",
    "                            if len(label) == 0:\n",
    "                                label = np.load(path + p + '/' + lab)\n",
    "                            else:\n",
    "                                label = np.concatenate([label, np.load(path + p + '/' + lab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6096\n"
     ]
    }
   ],
   "source": [
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare image data\n",
    "img_names = tf.train.match_filenames_once('../frames/train/house/*/*hand/Image*.png')\n",
    "img_queue = tf.train.string_input_producer(img_names)\n",
    "\n",
    "img_reader = tf.WholeFileReader()\n",
    "_, img_value = img_reader.read(img_queue)\n",
    "\n",
    "raw_img = tf.image.decode_png(img_value, channels = 3)\n",
    "raw_img = tf.image.resize_images(raw_img, [300, 300])\n",
    "raw_img.set_shape((300, 300, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6096\n"
     ]
    }
   ],
   "source": [
    "image = []\n",
    "init_op = (tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "#     print(sess.run(img_names))\n",
    "    \n",
    "    for i in range(img_num):\n",
    "        image_tensor = sess.run([raw_img])   \n",
    "        image += [image_tensor[0]]\n",
    "#         print(image_tensor[0].shape)\n",
    "    \n",
    "    # Finish off the filename queue coordinator.\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "print(len(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#State where your log file is at. If it doesn't exist, create it.\n",
    "log_dir = '../log'\n",
    "\n",
    "#State where your checkpoint file is\n",
    "checkpoint_file = '../inception_resnet_v2_2016_08_30.ckpt'\n",
    "\n",
    "#State the number of classes to predict:\n",
    "obj = { 'free':0,\n",
    "        'computer':1,\n",
    "        'cellphone':2,\n",
    "        'coin':3,\n",
    "        'ruler':4,\n",
    "        'thermos-bottle':5,\n",
    "        'whiteboard-pen':6,\n",
    "        'whiteboard-eraser':7,\n",
    "        'pen':8,\n",
    "        'cup':9,\n",
    "        'remote-control-TV':10,\n",
    "        'remote-control-AC':11,\n",
    "        'switch':12,\n",
    "        'windows':13,\n",
    "        'fridge':14,\n",
    "        'cupboard':15,\n",
    "        'water-tap':16,\n",
    "        'toy':17,\n",
    "        'kettle':18,\n",
    "        'bottle':19,\n",
    "        'cookie':20,\n",
    "        'book':21,\n",
    "        'magnet':22,\n",
    "        'lamp-switch':23}\n",
    "\n",
    "num_classes = len(obj)\n",
    "\n",
    "\n",
    "#================= TRAINING INFORMATION ==================\n",
    "#State the number of epochs to train\n",
    "num_epochs = 1\n",
    "\n",
    "#State your batch size\n",
    "batch_size = 8\n",
    "\n",
    "#Learning rate information and configuration (Up to you to experiment)\n",
    "initial_learning_rate = 0.0002\n",
    "learning_rate_decay_factor = 0.7\n",
    "num_epochs_before_decay = 2\n",
    "\n",
    "\n",
    "def load_batch(raw_image, label, batch_size, height=800, width=800, is_training=True):\n",
    "    '''\n",
    "    Loads a batch for training.\n",
    "\n",
    "    INPUTS:\n",
    "    - dataset(Dataset): a Dataset class object that is created from the get_split function\n",
    "    - batch_size(int): determines how big of a batch to train\n",
    "    - height(int): the height of the image to resize to during preprocessing\n",
    "    - width(int): the width of the image to resize to during preprocessing\n",
    "    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\n",
    "\n",
    "    OUTPUTS:\n",
    "    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n",
    "    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n",
    "\n",
    "    '''\n",
    "    #Perform the correct preprocessing for this image depending if it is training or evaluating\n",
    "    image = inception_preprocessing.preprocess_image(raw_image, height, width, is_training)\n",
    "\n",
    "    #As for the raw images, we just do a simple reshape to batch it up\n",
    "    raw_image = tf.expand_dims(raw_image, 0)\n",
    "    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n",
    "    raw_image = tf.squeeze(raw_image)\n",
    "\n",
    "    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\n",
    "    images, raw_image, labels = tf.train.batch(\n",
    "        [image, raw_image, label],\n",
    "        batch_size = batch_size,\n",
    "        num_threads = 4,\n",
    "        capacity = 4 * batch_size,\n",
    "        allow_smaller_final_batch = True)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    num_batches_per_epoch = int(img_num / batch_size)\n",
    "    num_steps_per_epoch = num_batches_per_epoch\n",
    "    decay_steps = int(num_epochs_before_decay * num_steps_per_epoch)\n",
    "\n",
    "    #Create the model inference\n",
    "    with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
    "        logits, end_points = inception_resnet_v2(image, \n",
    "                                                 num_classes = num_classes, \n",
    "                                                 is_training = True)\n",
    "\n",
    "    #Define the scopes that you want to exclude for restoration\n",
    "    exclude = ['InceptionResnetV2/Logits', \n",
    "               'InceptionResnetV2/AuxLogits']\n",
    "    variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n",
    "\n",
    "    one_hot_labels = slim.one_hot_encoding(label, \n",
    "                                           num_classes)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, \n",
    "                                           logits = logits)\n",
    "    total_loss = tf.losses.get_total_loss()\n",
    "\n",
    "    global_step = get_or_create_global_step()\n",
    "\n",
    "    lr = tf.train.exponential_decay(\n",
    "        learning_rate = initial_learning_rate,\n",
    "        global_step = global_step,\n",
    "        decay_steps = decay_steps,\n",
    "        decay_rate = learning_rate_decay_factor,\n",
    "        staircase = True)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    predictions = tf.argmax(end_points['Predictions'], 1)\n",
    "    probabilities = end_points['Predictions']\n",
    "    accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
    "    metrics_op = tf.group(accuracy_update, probabilities)\n",
    "    \n",
    "    print('Stage 1')\n",
    "    \n",
    "    #Now finally create all the summaries you need to monitor and group them into one summary op.\n",
    "    tf.summary.scalar('losses/Total_Loss', total_loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "    my_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    #Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.\n",
    "    def train_step(sess, train_op, global_step):\n",
    "        #Check the time for each sess run\n",
    "        start_time = time.time()\n",
    "        total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n",
    "        time_elapsed = time.time() - start_time\n",
    "\n",
    "        logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\n",
    "\n",
    "        return total_loss, global_step_count\n",
    "\n",
    "    #Now we create a saver function that actually restores the variables from a checkpoint file in a sess\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    def restore_fn(sess):\n",
    "        return saver.restore(sess, checkpoint_file)\n",
    "\n",
    "    #Define your supervisor for running a managed session. Do not run the summary_op automatically or else it will consume too much memory\n",
    "    sv = tf.train.Supervisor(logdir = log_dir, summary_op = None, init_fn = restore_fn)\n",
    "    \n",
    "    print('Stage 2')\n",
    "\n",
    "    #Run the managed session\n",
    "    with sv.managed_session() as sess:\n",
    "        for step in xrange(num_steps_per_epoch * num_epochs):\n",
    "            if step % num_batches_per_epoch == 0:\n",
    "                logging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, num_epochs)\n",
    "                learning_rate_value, accuracy_value = sess.run([lr, accuracy])\n",
    "                logging.info('Current Learning Rate: %s', learning_rate_value)\n",
    "                logging.info('Current Streaming Accuracy: %s', accuracy_value)\n",
    "\n",
    "                # optionally, print your logits and predictions for a sanity check that things are going fine.\n",
    "                logits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\n",
    "                print('logits: \\n', logits_value)\n",
    "                print('Probabilities: \\n', probabilities_value)\n",
    "                print('predictions: \\n', predictions_value)\n",
    "                print('Labels: \\n', labels_value)\n",
    "\n",
    "            #Log the summaries every 10 step.\n",
    "            if step % 10 == 0:\n",
    "                loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "                summaries = sess.run(my_summary_op)\n",
    "                sv.summary_computed(sess, summaries)\n",
    "\n",
    "            #If not, simply run the training step\n",
    "            else:\n",
    "                loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "\n",
    "        #We log the final training loss and accuracy\n",
    "        logging.info('Final Loss: %s', loss)\n",
    "        logging.info('Final Accuracy: %s', sess.run(accuracy))\n",
    "\n",
    "        #Once all the training has been done, save the log files and checkpoint model\n",
    "        logging.info('Finished training! Saving model to disk now.')\n",
    "        saver.save(sess, \"../model.ckpt\")\n",
    "        sv.saver.save(sess, sv.save_path, global_step = sv.global_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
