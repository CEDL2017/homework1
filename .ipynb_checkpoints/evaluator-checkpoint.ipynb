{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "import inception_preprocessing\n",
    "from inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11770\n"
     ]
    }
   ],
   "source": [
    "# prepare image\n",
    "\n",
    "place = ['house', 'lab', 'office']\n",
    "# place = ['house']\n",
    "num = ['1', '2', '3']\n",
    "# num = num[0:1]\n",
    "kind = ['Lhand', 'Rhand']\n",
    "# kind = kind[0:1]\n",
    "img_num = 0\n",
    "\n",
    "for p in place:\n",
    "    for n in num:\n",
    "        for k in kind:\n",
    "            path = '../frames/test/' + p + '/' + n + '/' + k\n",
    "            png_list = os.listdir(path)\n",
    "            img_num += len(png_list)\n",
    "            for pic in png_list:\n",
    "                npic = pic.replace('Image', '')\n",
    "                npic = npic.replace('.png', '')\n",
    "                if int(npic) < 100:\n",
    "                    npic = 'Image' + npic.zfill(3) + '.png'\n",
    "                    os.rename(path + '/' + pic, path + '/' + npic)\n",
    "\n",
    "# for k in kind:\n",
    "#     path = '../frames/train/' + 'lab' + '/' + '4' + '/' + k\n",
    "#     png_list = os.listdir(path)\n",
    "#     img_num += len(png_list)\n",
    "#     for pic in png_list:\n",
    "#         npic = pic.replace('Image', '')\n",
    "#         npic = npic.replace('.png', '')\n",
    "#         if int(npic) < 100:\n",
    "#             npic = 'Image' + npic.zfill(3) + '.png'\n",
    "#             os.rename(path + '/' + pic, path + '/' + npic)\n",
    "\n",
    "print(img_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11770\n"
     ]
    }
   ],
   "source": [
    "# prepare label\n",
    "# place = ['house', 'lab', 'office']\n",
    "num = ['4', '5', '6']\n",
    "# num = num[0:1]\n",
    "kind = ['left', 'right']\n",
    "# kind = kind[0:1]\n",
    "\n",
    "path = '../labels/'\n",
    "\n",
    "label = np.array([])\n",
    "\n",
    "for p in place:\n",
    "    lab_list = os.listdir(path + p)\n",
    "    for lab in lab_list:\n",
    "        ali = lab\n",
    "        if 'obj' in ali:\n",
    "            ali = ali.replace('.npy', '')\n",
    "            for k in kind:\n",
    "                if k in ali:\n",
    "                    if p == 'lab':\n",
    "                        for n in range(5, 8):\n",
    "                            if str(n) in ali:\n",
    "                                if len(label) == 0:\n",
    "                                    label = np.load(path + p + '/' + lab)\n",
    "                                else:\n",
    "                                    label = np.concatenate([label, np.load(path + p + '/' + lab)])\n",
    "                    else:\n",
    "                        for n in num:\n",
    "                            if n in ali:\n",
    "                                if len(label) == 0:\n",
    "                                    label = np.load(path + p + '/' + lab)\n",
    "                                else:\n",
    "                                    label = np.concatenate([label, np.load(path + p + '/' + lab)])\n",
    "label = label.astype(int)\n",
    "# label = label[0:2]\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../log\\model.ckpt-18964\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:global_step/sec: inf\n",
      "Prediction: free \n",
      " Ground Truth: free\n",
      "Prediction: free \n",
      " Ground Truth: free\n",
      "Prediction: free \n",
      " Ground Truth: free\n",
      "Prediction: free \n",
      " Ground Truth: free\n",
      "Prediction: free \n",
      " Ground Truth: free\n",
      "Prediction: free \n",
      " Ground Truth: free\n",
      "Prediction: free \n",
      " Ground Truth: free\n",
      "Prediction: free \n",
      " Ground Truth: free\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'TypeError'>, Fetch argument array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int64) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int64) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    266\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 267\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    268\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_finalized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2581\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2672\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\"\n\u001b[1;32m-> 2673\u001b[1;33m                       % (type(obj).__name__, types_str))\n\u001b[0m\u001b[0;32m   2674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a ndarray into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-58172ee0e37c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;31m#Now we want to visualize the last batch's images just to see what our model has predicted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             \u001b[0mpred_raw_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_raw_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_raw_image\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    982\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m--> 984\u001b[1;33m         self._graph, fetches, feed_dict_string, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    986\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    408\u001b[0m     \"\"\"\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \"\"\"\n\u001b[0;32m    336\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \"\"\"\n\u001b[0;32m    336\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    269\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[0;32m    270\u001b[0m                         \u001b[1;34m'must be a string or Tensor. (%s)'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m                         % (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[0;32m    272\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int64) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "log_dir = '../log'\n",
    "log_eval = '../log_eval_test'\n",
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "checkpoint_file = tf.train.latest_checkpoint(log_dir)\n",
    "\n",
    "#State the number of classes to predict:\n",
    "obj = { 'free':0,\n",
    "        'computer':1,\n",
    "        'cellphone':2,\n",
    "        'coin':3,\n",
    "        'ruler':4,\n",
    "        'thermos-bottle':5,\n",
    "        'whiteboard-pen':6,\n",
    "        'whiteboard-eraser':7,\n",
    "        'pen':8,\n",
    "        'cup':9,\n",
    "        'remote-control-TV':10,\n",
    "        'remote-control-AC':11,\n",
    "        'switch':12,\n",
    "        'windows':13,\n",
    "        'fridge':14,\n",
    "        'cupboard':15,\n",
    "        'water-tap':16,\n",
    "        'toy':17,\n",
    "        'kettle':18,\n",
    "        'bottle':19,\n",
    "        'cookie':20,\n",
    "        'book':21,\n",
    "        'magnet':22,\n",
    "        'lamp-switch':23}\n",
    "\n",
    "num_classes = len(obj)\n",
    "obj_by_value = {}\n",
    "\n",
    "for key in obj.keys():\n",
    "    obj_by_value[obj[key]] = key\n",
    "\n",
    "#Create log_dir for evaluation information\n",
    "if not os.path.exists(log_eval):\n",
    "    os.mkdir(log_eval)\n",
    "\n",
    "#Just construct the graph from scratch again\n",
    "with tf.Graph().as_default() as graph:\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "    #Create some information about the training steps\n",
    "    num_batches_per_epoch = int(img_num / batch_size)\n",
    "    num_steps_per_epoch = num_batches_per_epoch\n",
    "    \n",
    "    # prepare image FIFOQueue\n",
    "    height = 300\n",
    "    width = 300\n",
    "    num_threads = 4\n",
    "\n",
    "    image_names = tf.train.match_filenames_once('../frames/test/house/*/*hand/Image*.png')\n",
    "    image_queue = tf.train.string_input_producer(image_names)\n",
    "\n",
    "    image_reader = tf.WholeFileReader()\n",
    "    _, image_value = image_reader.read(image_queue)\n",
    "\n",
    "    image_tf = tf.image.decode_png(image_value, channels = 3)\n",
    "    image_tf = tf.image.resize_images(image_tf, [height, width])\n",
    "    image_tf.set_shape((height, width, 3))\n",
    "\n",
    "    image_tf = inception_preprocessing.preprocess_image(image_tf, height, width, is_training = True)\n",
    "\n",
    "    # prepare label FIFOQueue\n",
    "    label_tf = tf.convert_to_tensor(label, dtype = tf.int32)\n",
    "    label_queue = tf.train.slice_input_producer([label_tf], shuffle = False)\n",
    "    \n",
    "    # prepare original image for visualize\n",
    "    raw_image = tf.image.decode_png(image_value, channels = 3)\n",
    "    raw_image = tf.expand_dims(raw_image, 0)\n",
    "    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n",
    "    raw_image = tf.squeeze(raw_image)\n",
    "    \n",
    "    # creating a batch of images and labels\n",
    "    batch_image, batch_raw_image, batch_label = tf.train.batch([[image_tf], [raw_image], label_queue], \n",
    "                                                                batch_size = batch_size, \n",
    "                                                                num_threads = num_threads,\n",
    "                                                                enqueue_many = True,\n",
    "                                                                allow_smaller_final_batch = True)\n",
    "    \n",
    "    \n",
    "    #Now create the inference model but set is_training=False\n",
    "    with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
    "        logits, end_points = inception_resnet_v2(batch_image, num_classes = num_classes, is_training = True)\n",
    "\n",
    "    #get all the variables to restore from the checkpoint file and create the saver function to restore\n",
    "    variables_to_restore = slim.get_variables_to_restore()\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    def restore_fn(sess):\n",
    "        return saver.restore(sess, checkpoint_file)\n",
    "\n",
    "    #Just define the metrics to track without the loss or whatsoever\n",
    "    predictions = tf.argmax(end_points['Predictions'], 1)\n",
    "    accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, batch_label)\n",
    "    metrics_op = tf.group(accuracy_update)\n",
    "\n",
    "    #Create the global step and an increment op for monitoring\n",
    "    global_step = get_or_create_global_step()\n",
    "    global_step_op = tf.assign(global_step, global_step + 1) #no apply_gradient method so manually increasing the global_step\n",
    "\n",
    "\n",
    "    #Create a evaluation step function\n",
    "    def eval_step(sess, metrics_op, global_step):\n",
    "        start_time = time.time()\n",
    "        _, global_step_count, accuracy_value = sess.run([metrics_op, global_step_op, accuracy])\n",
    "        time_elapsed = time.time() - start_time\n",
    "\n",
    "        #Log some information\n",
    "        logging.info('Global Step %s: Streaming Accuracy: %.4f (%.2f sec/step)', global_step_count, accuracy_value, time_elapsed)\n",
    "\n",
    "        return accuracy_value\n",
    "\n",
    "\n",
    "    #Define some scalar quantities to monitor\n",
    "    tf.summary.scalar('Validation_Accuracy', accuracy)\n",
    "    my_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    #Get your supervisor\n",
    "    sv = tf.train.Supervisor(logdir = log_eval, summary_op = None, saver = None, init_fn = restore_fn)\n",
    "\n",
    "    #Now we are ready to run in one session\n",
    "    with sv.managed_session() as sess:\n",
    "#         for step in range(num_steps_per_epoch * num_epochs):\n",
    "#             sess.run(sv.global_step)\n",
    "#             #print vital information every start of the epoch as always\n",
    "#             if step % num_batches_per_epoch == 0:\n",
    "#                 logging.info('Epoch: %s/%s', step / num_batches_per_epoch + 1, num_epochs)\n",
    "#                 logging.info('Current Streaming Accuracy: %.4f', sess.run(accuracy))\n",
    "\n",
    "#             #Compute summaries every 10 steps and continue evaluating\n",
    "#             if step % 10 == 0:\n",
    "#                 eval_step(sess, metrics_op = metrics_op, global_step = sv.global_step)\n",
    "#                 summaries = sess.run(my_summary_op)\n",
    "#                 sv.summary_computed(sess, summaries)\n",
    "\n",
    "#             #Otherwise just run as per normal\n",
    "#             else:\n",
    "#                 eval_step(sess, metrics_op = metrics_op, global_step = sv.global_step)\n",
    "\n",
    "#         #At the end of all the evaluation, show the final accuracy\n",
    "#         logging.info('Final Streaming Accuracy: %.4f', sess.run(accuracy))\n",
    "\n",
    "        #Now we want to visualize the last batch's images just to see what our model has predicted\n",
    "        for step in range(num_steps_per_epoch):\n",
    "            pred_raw_image, pred_label, predicts = sess.run([batch_raw_image, batch_label, predictions])\n",
    "            for i in range(batch_size):\n",
    "                image, label, prediction = pred_raw_image[i], pred_label[i], predicts[i]\n",
    "                prediction_name, label_name = obj_by_value[int(predicts)], obj_by_value[int(label)]\n",
    "                text = 'Prediction: %s \\n Ground Truth: %s' %(prediction_name, label_name)\n",
    "                print(text)\n",
    "#             img_plot = plt.imshow(image)\n",
    "\n",
    "#             #Set up the plot and hide axes\n",
    "#             plt.title(text)\n",
    "#             img_plot.axes.get_yaxis().set_ticks([])\n",
    "#             img_plot.axes.get_xaxis().set_ticks([])\n",
    "#             plt.show()\n",
    "\n",
    "        logging.info('Model evaluation has completed! Visit TensorBoard for more information regarding your evaluation.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
